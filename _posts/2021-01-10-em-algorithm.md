---
layout: post
title: "[ML/Stat] EM Algorithm for latent variable models (1)"
categories: doc
tags: [ml, stat]
comments: true
use_math: true
---


ë‹¤ìŒ ê°•ì˜ë“¤ì„ ì°¸ê³ í•˜ì—¬ ê°œì¸ì ìœ¼ë¡œ EM ì•Œê³ ë¦¬ì¦˜ì— ëŒ€í•´ ê³µë¶€í•œ ë‚´ìš©ì…ë‹ˆë‹¤! ì‰½ì§€ ì•Šë„¤ìš” ğŸ˜…


- [Lecture 14 - Expectation-Maximization Algorithms \| Stanford CS229: Machine Learning (Autumn 2018)](https://www.youtube.com/watch?v=rVfZHWTwXSA&t=2192s)
- [27. EM Algorithm for Latent Variable Models](https://www.youtube.com/watch?v=lMShR1vjbUo)
    - [Lecture Slides](https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/13c.EM-algorithm.pdf)

# 1. EM Algorithm for latent variable models

EMì€ ê´€ì¸¡ë˜ì§€ ì•Šì€ ì ì¬ë³€ìˆ˜ê°€ ì¡´ì¬í•  ë•Œ í™•ë¥ ë¶„í¬ì˜ ìµœëŒ€ê°€ëŠ¥ë„ íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ëŠ” ì•Œê³ ë¦¬ì¦˜ì´ë‹¤. ì†”ì§íˆ ì´ë ‡ê²Œ ë“¤ì–´ì„œëŠ” ì˜ ê°ì´ ì˜¤ì§€ ì•ŠëŠ”ë‹¤. ì˜ˆì‹œë¥¼ í†µí•´ì„œ ì•Œì•„ë³´ì. ë‹¤ìŒê³¼ ê°™ì€ 1ì°¨ì› ë°ì´í„°ì˜ ë¶„í¬ë¥¼ ê°€ì •í•œë‹¤. í•˜ë‚˜ì˜ ë¶„í¬ë¥¼ í†µí•´ì„œ ì„¤ëª…í•  ìˆ˜ë„ ìˆê² ì§€ë§Œ, ë‘ ê°œì˜ ê°€ìš°ì‹œì•ˆì´ í˜¼í•©ëœ ë¶„í¬ë¡œ ì„¤ëª…í•˜ëŠ” í¸ì´ ë” ë‚˜ì•„ ë³´ì¸ë‹¤. \\( n \\) ê°œì˜ ë°ì´í„°ê°€ ì¡´ì¬í•˜ê³  í™•ë¥ ë³€ìˆ˜ \\( X, Z \\)ê°€ ê°ê° ì•Œë ¤ì§„ ê°’ê³¼ ìˆ¨ê²¨ì§„ í´ëŸ¬ìŠ¤í„°(ê°€ìš°ì‹œì•ˆ ë¶„í¬)ì— ëŒ€ì‘í•œë‹¤ê³  í•´ë³´ì. \\( X \\)ëŠ” ì„ì˜ì˜ ì‹¤ìˆ˜ ê°’ì„ ê°€ì§€ë©° \\( Z \\) ëŠ” 0 ë˜ëŠ” 1ì˜ ê°’ì„ ê°€ì§„ë‹¤. ì•„ë˜ ê·¸ë¦¼ì€ \\( N(1, 1^2) \\) ì˜ ìƒ˜í”Œ 100ê°œ, \\( N(10, 3^2) \\) ì˜ ìƒ˜í”Œ 50ê°œë¡œ ì´ë£¨ì–´ì§„ ë°ì´í„°ì˜ ë¶„í¬ì´ë‹¤.

    
![png](/assets/img/docs/output_4_0.png)


ì™„ì „í•œ ë°ì´í„°(complete data) \\( \{(x_1, z_1), (x_2, z_2), \space ... \space, (x_n, z_n)\} \\) ê°€ ì¡´ì¬í•œë‹¤ë©´ ë‘ ê°œì˜ ë¶„í¬ì— ëŒ€í•´ì„œ ê°ê° MLEë¥¼ ì¶”ì •í•  ìˆ˜ ìˆë‹¤. ì¦‰ ì–´ë–¤ ë°ì´í„°ê°€ ì–´ë–¤ ê°€ìš°ì‹œì•ˆ ë¶„í¬ë¡œë¶€í„° ë‚˜ì™”ëŠ”ì§€ë¥¼ ì•„ëŠ” ìƒíƒœì´ë‹¤. í•˜ì§€ë§Œ ì‹¤ì œë¡œ ê°€ì§„ ë°ì´í„°ëŠ” \\( \{x_1, x_2, \space ... \space , x_n\} \\) ë¿ì´ë‹¤. ê°ê°ì´ ì–´ë–¤ ë¶„í¬ë¡œë¶€í„° ë‚˜ì™”ëŠ”ì§€ë¥¼ ì•Œ ìˆ˜ ì—†ëŠ” ê²ƒì´ë‹¤. ì´ë¥¼ ë¶ˆì™„ì „í•œ ë°ì´í„°(incomplete data)ë¼ê³  ë¶€ë¥¸ë‹¤. \\( X \\) ì™€ \\( Z \\)ì˜ ê²°í•©í™•ë¥ ë¶„í¬ ëª¨ë¸ì´ íŒŒë¼ë¯¸í„° \\( \theta \\)ë¥¼ ê°€ì§„ë‹¤ê³  í•˜ë©´ ëª¨ë¸ì„ \\( p(x, z \vert \theta) \\)ì™€ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.

- ì‹¤ì œë¡œ ê´€ì¸¡ëœ í™•ë¥ ë³€ìˆ˜ \\( X: x_1, x_2, ... x_n \\)
- \\( X \\) ì— ëŒ€ì‘í•˜ëŠ” ì ì¬ë³€ìˆ˜ \\( Z: z_1, z_2, ... z_n \\)
- Complete data: \\( \{(x_1, z_1), (x_2, z_2), \space ... \space, (x_n, z_n)\} \\) 
- Incomplete data: \\( \{x_1, x_2, \space ... \space , x_n\} \\)
- \\( X \\) ì™€ \\( Z \\)ì˜ ê²°í•©í™•ë¥ ë¶„í¬ ëª¨ë¸: \\( p(x, z \vert \theta) \\)

# 2. ELBO

ìš°ë¦¬ì˜ ëª©ì ì€ ë°ì´í„°ì˜ ë¡œê·¸ ê°€ëŠ¥ë„ë¥¼ ìµœëŒ€í™”í•˜ëŠ” íŒŒë¼ë¯¸í„° \\( \theta \\) ë¥¼ ì°¾ëŠ” ê²ƒì´ë‹¤. ì´ ë•Œ í•˜ë‚˜ì˜ ë¶„í¬ë§Œìœ¼ë¡œëŠ” ë°ì´í„°ë¥¼ ì„¤ëª…í•˜ê¸° ì–´ë µê¸° ë•Œë¬¸ì— ì ì¬ë³€ìˆ˜ \\( Z \\)ë¥¼ ë„ì…í•˜ì—¬ ê°€ìš°ì‹œì•ˆ í˜¼í•© ëª¨í˜•ì„ ì „ì œí•˜ê³ ì í•œë‹¤. \\( X \\) ì™€ \\( Z \\)ì˜ ê²°í•©í™•ë¥ ë¶„í¬ ëª¨ë¸ì„ \\( p(x, z \vert \theta) \\)ë¼ê³  í‘œí˜„í•˜ì. ê·¸ë ‡ë‹¤ë©´ \\( p(x) \\)ëŠ” ê²°í•©ë¶„í¬ì—ì„œ ì£¼ë³€í•©ì„ êµ¬í•œ í˜•íƒœë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤. ì´ëŸ¬í•œ í¼ì„ ë² ì´ì¦ˆ ì •ë¦¬ì—ì„œ **Evidence** ë¼ê³  ë¶€ë¥¸ë‹¤. í•˜ì§€ë§Œ ëŒ€ë¶€ë¶€ì˜ ê²½ìš° **Evidence** ë¥¼ ì§ì ‘ ìµœëŒ€í™”í•˜ëŠ” ê²ƒì€ ì–´ë µë‹¤. ê·¸ë˜ì„œ **Evidenceì˜ í•˜í•œì¸ ELBO(Evidence Lower Bound)ë¥¼ ìµœëŒ€í™”í•˜ëŠ” ë°©ì‹ì„ íƒí•œë‹¤.** ì´í›„ **ELBOë¥¼ ìµœëŒ€í™”í•˜ëŠ” íŒŒë¼ë¯¸í„°ëŠ” ê³§ Evidence ë¥¼ ìµœëŒ€í™”í•˜ëŠ” íŒŒë¼ë¯¸í„°ì™€ ê°™ë‹¤**ëŠ” ê²ƒì„ ë³´ì¼ ìˆ˜ ìˆë‹¤. ë°ì´í„° ì „ì²´ì— ëŒ€í•œ í•©ì€ ìƒëµí•˜ê³  ì ì–´ë³´ë„ë¡ í•˜ì.

$$
\begin{align} 
arg \space max_{\theta} \log p(x \vert \theta) \\
p(x \vert \theta) = \sum_{z}p(x, z \vert \theta)\\
\end{align}
$$

ì ì¬ë³€ìˆ˜ \\( Z \\)ì˜ PMFë¥¼ \\( q \\)ë¼ í•˜ë©´ ELBOëŠ” ë‹¤ìŒê³¼ ê°™ì´ ìœ ë„í•  ìˆ˜ ìˆë‹¤. ë¡œê·¸ëŠ” ì˜¤ëª©(concave)í•¨ìˆ˜ì´ë¯€ë¡œ ì  ìŠ¨ ë¶€ë“±ì‹ì— ì˜í•´ ë¡œê·¸ì˜ ê¸°ëŒ€ê°’ì€ ê¸°ëŒ€ê°’ì˜ ë¡œê·¸ë³´ë‹¤ ì‘ê±°ë‚˜ ê°™ë‹¤. ë°ì´í„° \\( x \\)ê°€ ì£¼ì–´ì§„ ìƒí™©ì—ì„œ ELBOëŠ” \\( q \\)ì™€ \\( \theta \\) ì˜ í•¨ìˆ˜ì´ë¯€ë¡œ \\( \mathcal{L}(q, \theta) \\) ë¼ í•˜ì. ì–´ë–¤ PMF \\( q(z) \\)ì— ëŒ€í•´ì„œë„ \\( \mathcal{L}(q, \theta) \\) ëŠ” Evidence ë³´ë‹¤ ì‘ê±°ë‚˜ ê°™ë‹¤. ëª©ì í•¨ìˆ˜ë¥¼ ì •ì˜í•˜ì˜€ìœ¼ë¯€ë¡œ ì´ì œ ì´ë¥¼ ìµœëŒ€í™”í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì„ ê¸°ìˆ í•´ë³´ë„ë¡í•˜ì.

$$
\begin{align} 
\log p(x \vert \theta) &= \log \left[ \sum_{z}p(x, z \vert \theta) \right] \\ &=
\log \left[ \sum_{z} q(z) \frac{p(x, z \vert \theta)}{q(z)}) \right] \quad \text{log of an expectation} \\  & \geq
\sum_{z} q(z) \left[ \log \frac{p(x, z \vert \theta)}{q(z)}) \right] \quad \text{expectation of log, by Jensen's inequality}
\end{align} \\
\therefore \text{For any q(z), } \log p(x \vert \theta) \geq \mathcal{L}(q, \theta) 
$$

# 3. Expectation-Maximization

EM ì•Œê³ ë¦¬ì¦˜ì€ ë‹¤ìŒê³¼ ê°™ì´ ì§„í–‰ëœë‹¤.

1. íŒŒë¼ë¯¸í„° \\( \theta^{old} \\) ë¥¼ ëœë¤í•˜ê²Œ ì´ˆê¸°í™”í•œë‹¤.
2. \\( q^{\ast} = arg \space max_{q} \mathcal{L}(q, \theta^{old}) \\) ë¥¼ ì°¾ëŠ”ë‹¤.
3. \\( \theta^{new} = arg \space max_{\theta} \mathcal{L}(q*, \theta) \\) ë¥¼ ì°¾ëŠ”ë‹¤.
4. ë¡œê·¸ìš°ë„ê°€ ìˆ˜ë ´í•˜ì§€ ì•Šì•˜ë‹¤ë©´ \\( \theta^{old} \leftarrow \theta^{new} \\) ë¡œ ë†“ê³  2ë²ˆìœ¼ë¡œ ëŒì•„ê°„ë‹¤.

ì—¬ê¸°ì—ì„œ ì–¸ì œë‚˜ \\( \log p(x \vert \theta^{new}) \geq \log p(x \vert \theta^{old}) \\) ì„ì„ ë³´ì¼ ìˆ˜ ìˆë‹¤. **ì¦‰ ì•Œê³ ë¦¬ì¦˜ì˜ ì§„í–‰ì— ë”°ë¼ ë¡œê·¸ìš°ë„ëŠ” ë‹¨ì¡°ì¦ê°€í•œë‹¤.** ê·¸ ì´ìœ ë¥¼ ì•Œê¸° ìœ„í•´ì„œ ELBOì˜ ìˆ˜ì‹ì„ ì¡°ê¸ˆ ë‹¤ë¥¸ í˜•íƒœë¡œ ëœ¯ì–´ë³¼ í•„ìš”ê°€ ìˆë‹¤.

## 3.1. Maximizing over \\( q \\) for fixed \\( \theta = \theta^{old} \\)

ì•Œê³ ë¦¬ì¦˜ì˜ 2ë²ˆ ìŠ¤í…ì—ì„œëŠ” ê³ ì •ëœ \\( \theta^{old} \\) ë¥¼ í†µí•´ ELBOë¥¼ ìµœëŒ€í™”í•˜ëŠ” PMF \\( q(z) \\)ë¥¼ ì°¾ëŠ”ë‹¤. ELBO ìˆ˜ì‹ì„ ì•½ê°„ ë³€í˜•í•˜ë©´ Negative KL Divergence ì™€ Evidenceì˜ í•©ìœ¼ë¡œ ë¶„í•´ëœë‹¤. Evidence í•­ì€ \\( z \\)ë¥¼ ì£¼ë³€í™”ì‹œí‚¨ í™•ë¥ ì´ë¯€ë¡œ \\( q \\)ê°€ ê´€ì—¬í•˜ì§€ ì•ŠëŠ”ë‹¤. ì¦‰ ìš°ë¦¬ëŠ” KL Divergenceë¥¼ ìµœì†Œí™”í•˜ëŠ” \\( q \\)ë¥¼ ì°¾ëŠ” ê²ƒì´ë‹¤. KL DivergenceëŠ” ë‘ ë¶„í¬ê°€ ê°™ì„ ë•Œ ìµœì†Œê°’ì„ ê°€ì§€ë¯€ë¡œ \\( q(z) = p(z \vert x, \theta^{old}) \\) ì´ê³  ì´ë•Œ KL Divergenceì˜ ê°’ì€ 0ì´ë¯€ë¡œ ê²°êµ­ \\( \mathcal{ L }(q^{\ast}, \theta^{old}) = \log p(x \vert \theta) \\) ê°€ ëœë‹¤. ì•ì—ì„œ ELBOëŠ” **Evidenceì˜ í•˜í•œ**ì´ë¼ê³  í–ˆë‹¤. ë”°ë¼ì„œ \\( q^{\ast} \\) ì—ì„œ ELBOì™€ Evidenceì˜ ê°’ì´ ê°™ì•„ì§„ë‹¤ëŠ” ê²ƒì€ \\( q^{\ast} \\) ì—ì„œ ë‘ í•¨ìˆ˜ê°€ ì ‘í•œë‹¤ëŠ” ëœ»ì´ ëœë‹¤.

$$
\begin{align}
\mathcal{L}(q, \theta) &=
 \sum_{z} q(z) \left[ \log \frac{p(x, z \vert \theta)}{q(z)} \right] \\ &=
 \sum_{z} q(z) \left[ \log \frac{p(z \vert x, \theta) p(x \vert \theta)}{q(z)}) \right] (\text{conditional probability})\\ &=
 \sum_{z} q(z) \log \frac{p(z \vert x, \theta)}{q(z)}) + \sum_{z} q(z)\log p(x \vert \theta) \\ &=
  -KL \left[ q(z) \vert \vert p(z \vert x, \theta) \right] + \log p(x \vert \theta)  \space
\end{align} \\
\therefore q^{\ast} = arg \space max_q \mathcal{L}(q, \theta^{old}) \implies \mathcal{L(q^{\ast}, \theta^{old})} = \log p(x \vert \theta^{old})
$$


## 3.2. Maximizing over \\( \theta \\) for fixed \\( q^{\ast} \\)


ë‹¤ìŒìœ¼ë¡œëŠ” \\( arg \space max_{theta} \mathcal{L}(q^{\ast}, \theta) \\) ë¥¼ ì°¾ì•„ì•¼ í•œë‹¤. ELBO ìˆ˜ì‹ì„ ë³€í˜•í•˜ë©´ \\( \theta \\)ê°€ ê´€ì—¬í•˜ì§€ ì•ŠëŠ” ë‘˜ì§¸ í•­ì€ ë¬´ì‹œí•  ìˆ˜ ìˆë‹¤. ì²«ì§¸ í•­ì€ \\( X \\)ì™€ \\( Y \\)ì˜ ê²°í•©ë¶„í¬, ì¦‰ ì™„ì „í•œ ë°ì´í„°(complete data)ì˜ ë¡œê·¸ìš°ë„ ê¸°ëŒ€ê°’ì´ë‹¤. ì¦‰ ELBOë¥¼ ìµœëŒ€í™”í•˜ëŠ” ê²ƒì€ ì™„ì „í•œ ë°ì´í„°ì˜ ë¡œê·¸ìš°ë„ ê¸°ëŒ€ê°’ì„ ìµœëŒ€í™”í•˜ëŠ” ê²ƒê³¼ ê°™ë‹¤. \\( q^{\ast} \\)ë¥¼ ì°¾ì•„ ì™„ì „í•œ ë°ì´í„°ì˜ ë¡œê·¸ìš°ë„ ê¸°ëŒ€ê°’ ìˆ˜ì‹ì„ ì„¸ìš°ëŠ” ê³¼ì •ì„ EìŠ¤í…, ì´ ê¸°ëŒ€ê°’ì„ ìµœëŒ€í™”í•˜ëŠ” \\( \theta^{new} \\) ë¥¼ ì°¾ëŠ” 3ë²ˆ ìŠ¤í…ì„ MìŠ¤í…ì´ë¼ê³  ë¶€ë¥¸ë‹¤. ì ì¬ë³€ìˆ˜ \\( Z \\)ì˜ ê°’ì„ ì •í™•íˆ ëª¨ë¥´ê¸° ë•Œë¬¸ì— ì£¼ì–´ì§„ PMF \\( q^{\ast}(z) \\) ì—ì„œì˜ ê¸°ëŒ€ê°’ì„ ì‚¬ìš©í•œë‹¤ê³  ë³´ë©´ ë  ê²ƒ ê°™ë‹¤.


$$
\begin{align}
\mathcal{L}(q, \theta) &=
 \sum_{z} q(z) \left[ \log \frac{p(x, z \vert \theta)}{q(z)} \right] \\ &=
 \underbrace { \sum_{z} q(z) \log p(x, z \vert \theta) }_{\text{expectation of complete data log-lokelihood}} - \underbrace { \sum_{z} q(z) \log q(z) }_{\text{no theta here}} \\ &
\therefore \theta^{new} = arg \space max_\theta \sum_{z} q^{\ast}(z) \log p(x, z \vert \theta)
\end{align}
$$


## 3.3. EM Gives Monotonically Increasing Likelihood

ì§€ê¸ˆê¹Œì§€ ë…¼ì˜í•œ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ë©´ ë‹¤ìŒ ë¶€ë“±ì‹ì´ ì„±ë¦½í•œë‹¤. ì¦‰ EM ì•Œê³ ë¦¬ì¦˜ì˜ ì´í„°ë ˆì´ì…˜ì— ë”°ë¼ì„œ ë¡œê·¸ìš°ë„ê°€ ë‹¨ì¡°ì¦ê°€í•¨ì„ ë³´ì¼ ìˆ˜ ìˆë‹¤.


$$
\begin{align}
\log p(x \vert \theta^{new}) &\geq   \mathcal{L}(q^{\ast}, \theta^{new}) \quad \mathcal{L}\text{ is a lower bound of evidence} \\ &\geq
\mathcal{L}(q^{\ast}, \theta^{old})\quad \text{By definition of } \theta^{new} \\ &=
\log p(x \vert \theta^{old}) \quad \text{Lower bound of evidence is tight at } q^{\ast}
\end{align}
$$


## 3.4. Global Maximum

ë˜í•œ ELBOì˜ ê¸€ë¡œë²Œ ë§¥ì‹œë©ˆ \\( \theta^\ast \\)ì´ ë¡œê·¸ìš°ë„ì˜ ê¸€ë¡œë²Œ ë§¥ì‹œë©ˆê³¼ ê°™ë‹¤ëŠ” ì‚¬ì‹¤ì„ ë³´ì¼ ìˆ˜ ìˆë‹¤. ELBOì˜ ê¸€ë¡œë²Œ ë§¥ì‹œë©ˆì„ \\( \mathcal{L}(q^{\ast}, \theta^\ast) \\) ë¼ í•˜ì. ì „ì—­ ìµœëŒ€ì´ë¯€ë¡œ ë‹¤ìŒì´ ì„±ë¦½í•œë‹¤. ìœ„ì—ì„œëŠ”  \\( q \\)ë¥¼ ì—…ë°ì´íŠ¸í•  ë•Œ \\( q^{\ast}(z) = p(z \vert x, \theta^{old}) \\) ë¡œ ì¼ì§€ë§Œ ê¸€ë¡œë²Œ ë§¥ì‹œë©ˆì—ì„œëŠ” ê²°êµ­ \\( \theta \\)ê°€ ê³ ì •ë˜ë¯€ë¡œ \\( q^{\ast}(z) = p(z \vert x, \theta^\ast) \\) ê°€ ëœë‹¤.


$$
\begin{align} 
\mathcal{L}(q^{\ast}, \theta^\ast) \geq \mathcal{L}(q, \theta), \space \forall (q, \theta) \quad where \quad q^{\ast}(z) = p(z \vert x, \theta^\ast)
\end{align} 
$$


ì´ ë•Œ \\( \theta^\ast \\)ì€  ELBO ë¿ë§Œ ì•„ë‹ˆë¼ Evidenceì˜ ì „ì—­ ìµœëŒ€ì ì´ê¸°ë„ í•˜ë‹¤. ì„ì˜ì˜ íŒŒë¼ë¯¸í„° \\( \thetaâ€² \\)ì™€ ì´ì— ëŒ€í•´ \\( qâ€²(z) = p(z \vert x, \thetaâ€²) \\) ì„ ë§Œì¡±í•˜ëŠ” \\( qâ€² \\) ë¥¼ ê°€ì •í•˜ì. ì´ ê²½ìš° Lower boundê°€ ì£¼ì–´ì§„ \\( \thetaâ€² \\) ì— ëŒ€í•´ ìµœëŒ€í™”ëœ ìƒíƒœë¡œ ë” ì´ìƒ ì¦ê°€í•  ìˆ˜ ì—†ìœ¼ë©° Evidenceì™€ ì ‘í•´ ìˆë‹¤. ì¦‰ ë‘˜ì€ ê°™ì€ ê°’ì„ ê°€ì§„ë‹¤. ì´í›„ ì „ì—­ ìµœëŒ€ì ì— ëŒ€í•´ì„œ \\( \mathcal{L}(q^{\ast}, \theta^\ast) \geq \mathcal{L}(q^âˆ—, \theta^âˆ—) \\) ê°€ ì„±ë¦½í•œë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ \\( \mathcal{L}(q^âˆ—, \theta^âˆ—) \\) ëŠ” íƒ€ì´íŠ¸í•œ ELBO ì´ë¯€ë¡œ Evidenceì™€ ê°™ë‹¤.


$$
\begin{align}
\log p(x|\thetaâ€²) &= \mathcal{L}(qâ€², \thetaâ€²) + KL[qâ€²,p(z|x, \thetaâ€²)] \\ &=
\mathcal{L}(qâ€²,\thetaâ€²) \\ & \leq
\mathcal{L}(q^âˆ—, \theta^âˆ—) \quad \text{Global maximum} \\ &=
\log p(x \vert \theta^âˆ—) \quad \text{Lower bound is tight}
\end{align} 
$$



## ì°¸ê³ ìë£Œ

- [Lecture 14 - Expectation-Maximization Algorithms \| Stanford CS229: Machine Learning (Autumn 2018)](https://www.youtube.com/watch?v=rVfZHWTwXSA&t=2192s)
- [27. EM Algorithm for Latent Variable Models](https://www.youtube.com/watch?v=lMShR1vjbUo)
