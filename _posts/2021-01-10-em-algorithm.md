---
layout: post
title: "[ML/Stat] EM Algorithm for latent variable models"
categories: doc
tags: [ml, stat]
comments: true
use_math: true
---


ë‹¤ìŒ ê°•ì˜ë“¤ì„ ì°¸ê³ í•˜ì—¬ ê°œì¸ì ìœ¼ë¡œ EM ì•Œê³ ë¦¬ì¦˜ì— ëŒ€í•´ ê³µë¶€í•œ ë‚´ìš©ìž…ë‹ˆë‹¤! ì‰½ì§€ ì•Šë„¤ìš” ðŸ˜…


- [Lecture 14 - Expectation-Maximization Algorithms \| Stanford CS229: Machine Learning (Autumn 2018)](https://www.youtube.com/watch?v=rVfZHWTwXSA&t=2192s)
- [27. EM Algorithm for Latent Variable Models](https://www.youtube.com/watch?v=lMShR1vjbUo)
    - [Lecture Slides](https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/13c.EM-algorithm.pdf)

# 1. EM Algorithm for latent variable models

EMì€ ê´€ì¸¡ë˜ì§€ ì•Šì€ ìž ìž¬ë³€ìˆ˜ê°€ ì¡´ìž¬í•  ë•Œ í™•ë¥ ë¶„í¬ì˜ ìµœëŒ€ê°€ëŠ¥ë„ íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ëŠ” ì•Œê³ ë¦¬ì¦˜ì´ë‹¤. ì†”ì§ížˆ ì´ë ‡ê²Œ ë“¤ì–´ì„œëŠ” ìž˜ ê°ì´ ì˜¤ì§€ ì•ŠëŠ”ë‹¤. ì˜ˆì‹œë¥¼ í†µí•´ì„œ ì•Œì•„ë³´ìž. ë‹¤ìŒê³¼ ê°™ì€ 1ì°¨ì› ë°ì´í„°ì˜ ë¶„í¬ë¥¼ ê°€ì •í•œë‹¤. í•˜ë‚˜ì˜ ë¶„í¬ë¥¼ í†µí•´ì„œ ì„¤ëª…í•  ìˆ˜ë„ ìžˆê² ì§€ë§Œ, ë‘ ê°œì˜ ê°€ìš°ì‹œì•ˆì´ í˜¼í•©ëœ ë¶„í¬ë¡œ ì„¤ëª…í•˜ëŠ” íŽ¸ì´ ë” ë‚˜ì•„ ë³´ì¸ë‹¤. $n$ ê°œì˜ ë°ì´í„°ê°€ ì¡´ìž¬í•˜ê³  í™•ë¥ ë³€ìˆ˜ $X, Z$ê°€ ê°ê° ì•Œë ¤ì§„ ê°’ê³¼ ìˆ¨ê²¨ì§„ í´ëŸ¬ìŠ¤í„°(ê°€ìš°ì‹œì•ˆ ë¶„í¬)ì— ëŒ€ì‘í•œë‹¤ê³  í•´ë³´ìž. $X$ëŠ” ìž„ì˜ì˜ ì‹¤ìˆ˜ ê°’ì„ ê°€ì§€ë©° $Z$ ëŠ” 0 ë˜ëŠ” 1ì˜ ê°’ì„ ê°€ì§„ë‹¤. 


```python
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
```


```python
np.random.seed(2021)

x1 = np.random.normal(1, 1, 100)
x2 = np.random.normal(10, 3, 50)
x = np.append(x1, x2)
np.random.shuffle(x)

sns.displot(x, kind="kde", rug=True)
```



![png](/assets/img/docs/output_4_1.png)


ì™„ì „í•œ ë°ì´í„°(complete data) $\{(x_1, z_1), (x_1, z_1), \space ... \space, (x_n, z_n)\}$ ê°€ ì¡´ìž¬í•œë‹¤ë©´ ë‘ ê°œì˜ ë¶„í¬ì— ëŒ€í•´ì„œ ê°ê° MLEë¥¼ ì¶”ì •í•  ìˆ˜ ìžˆë‹¤. ì¦‰ ì–´ë–¤ ë°ì´í„°ê°€ ì–´ë–¤ ê°€ìš°ì‹œì•ˆ ë¶„í¬ë¡œë¶€í„° ë‚˜ì™”ëŠ”ì§€ë¥¼ ì•„ëŠ” ìƒíƒœì´ë‹¤. í•˜ì§€ë§Œ ì‹¤ì œë¡œ ê°€ì§„ ë°ì´í„°ëŠ” $\{x_1, x_2, \space ... \space , x_n\}$ ë¿ì´ë‹¤. ê°ê°ì´ ì–´ë–¤ ë¶„í¬ë¡œë¶€í„° ë‚˜ì™”ëŠ”ì§€ë¥¼ ì•Œ ìˆ˜ ì—†ëŠ” ê²ƒì´ë‹¤. ì´ë¥¼ ë¶ˆì™„ì „í•œ ë°ì´í„°(incomplete data)ë¼ê³  ë¶€ë¥¸ë‹¤. $X$ ì™€ $Z$ì˜ ê²°í•©í™•ë¥ ë¶„í¬ ëª¨ë¸ì´ íŒŒë¼ë¯¸í„° $\theta$ë¥¼ ê°€ì§„ë‹¤ê³  í•˜ë©´ ëª¨ë¸ì„ $p(x, z \vert \theta)$ì™€ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìžˆë‹¤.

- ì‹¤ì œë¡œ ê´€ì¸¡ëœ í™•ë¥ ë³€ìˆ˜ $X: x_1, x_2, ... x_n$
- $X$ì— ëŒ€ì‘í•˜ëŠ” ìž ìž¬ë³€ìˆ˜ $Z: z_1, z_2, ... z_n$
- Complete data: $\{(x_1, z_1), (x_1, z_1), \space ... \space, (x_n, z_n)\}$ 
- Incomplete data: $\{x_1, x_2, \space ... \space , x_n\}$
- $X$ ì™€ $Z$ì˜ ê²°í•©í™•ë¥ ë¶„í¬ ëª¨ë¸: $p(x, z \vert \theta)$

# 2. ELBO

ìš°ë¦¬ì˜ ëª©ì ì€ ë°ì´í„°ì˜ ë¡œê·¸ ê°€ëŠ¥ë„ë¥¼ ìµœëŒ€í™”í•˜ëŠ” íŒŒë¼ë¯¸í„° $\theta$ ë¥¼ ì°¾ëŠ” ê²ƒì´ë‹¤. ì´ ë•Œ í•˜ë‚˜ì˜ ë¶„í¬ë§Œìœ¼ë¡œëŠ” ë°ì´í„°ë¥¼ ì„¤ëª…í•˜ê¸° ì–´ë µê¸° ë•Œë¬¸ì— ìž ìž¬ë³€ìˆ˜ $Z$ë¥¼ ë„ìž…í•˜ì—¬ ê°€ìš°ì‹œì•ˆ í˜¼í•© ëª¨í˜•ì„ ì „ì œí•˜ê³ ìž í•œë‹¤. $X$ ì™€ $Z$ì˜ ê²°í•©í™•ë¥ ë¶„í¬ ëª¨ë¸ì„ $p(x, z \vert \theta)$ë¼ê³  í‘œí˜„í•˜ìž. ê·¸ë ‡ë‹¤ë©´ $p(x)$ëŠ” ê²°í•©ë¶„í¬ì—ì„œ ì£¼ë³€í•©ì„ êµ¬í•œ í˜•íƒœë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìžˆë‹¤. ì´ëŸ¬í•œ í¼ì„ ë² ì´ì¦ˆ ì •ë¦¬ì—ì„œ **Evidence** ë¼ê³  ë¶€ë¥¸ë‹¤. í•˜ì§€ë§Œ ëŒ€ë¶€ë¶€ì˜ ê²½ìš° **Evidence** ë¥¼ ì§ì ‘ ìµœëŒ€í™”í•˜ëŠ” ê²ƒì€ ì–´ë µë‹¤. ê·¸ëž˜ì„œ **Evidenceì˜ í•˜í•œì¸ ELBO(Evidence Lower Bound)ë¥¼ ìµœëŒ€í™”í•˜ëŠ” ë°©ì‹ì„ íƒí•œë‹¤.** ì´í›„ **ELBOë¥¼ ìµœëŒ€í™”í•˜ëŠ” íŒŒë¼ë¯¸í„°ëŠ” ê³§ Evidence ë¥¼ ìµœëŒ€í™”í•˜ëŠ” íŒŒë¼ë¯¸í„°ì™€ ê°™ë‹¤**ëŠ” ê²ƒì„ ë³´ì¼ ìˆ˜ ìžˆë‹¤. ë°ì´í„° ì „ì²´ì— ëŒ€í•œ í•©ì€ ìƒëžµí•˜ê³  ì ì–´ë³´ë„ë¡ í•˜ìž.

$$
arg \space max_{\theta} \log p(x \vert \theta) \\
p(x \vert \theta) = \sum_{z}p(x, z \vert \theta)\\
$$

ìž ìž¬ë³€ìˆ˜ $Z$ì˜ PMFë¥¼ $q$ë¼ í•˜ë©´ ELBOëŠ” ë‹¤ìŒê³¼ ê°™ì´ ìœ ë„í•  ìˆ˜ ìžˆë‹¤. ë¡œê·¸ëŠ” ì˜¤ëª©(concave)í•¨ìˆ˜ì´ë¯€ë¡œ ì  ìŠ¨ ë¶€ë“±ì‹ì— ì˜í•´ ë¡œê·¸ì˜ ê¸°ëŒ€ê°’ì€ ê¸°ëŒ€ê°’ì˜ ë¡œê·¸ë³´ë‹¤ ìž‘ê±°ë‚˜ ê°™ë‹¤. ë°ì´í„° $x$ê°€ ì£¼ì–´ì§„ ìƒí™©ì—ì„œ ELBOëŠ” $q$ì™€ $\theta$ ì˜ í•¨ìˆ˜ì´ë¯€ë¡œ $\mathcal{L}(q, \theta)$ ë¼ í•˜ìž. ì–´ë–¤ PMF $q(z)$ì— ëŒ€í•´ì„œë„ $\mathcal{L}(q, \theta)$ ëŠ” Evidence ë³´ë‹¤ ìž‘ê±°ë‚˜ ê°™ë‹¤. ëª©ì í•¨ìˆ˜ë¥¼ ì •ì˜í•˜ì˜€ìœ¼ë¯€ë¡œ ì´ì œ ì´ë¥¼ ìµœëŒ€í™”í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì„ ê¸°ìˆ í•´ë³´ë„ë¡í•˜ìž.

$$
\begin{align} 
\log p(x \vert \theta) &= \log \left[ \sum_{z}p(x, z \vert \theta) \right] \\ &=
\log \left[ \sum_{z} q(z) \frac{p(x, z \vert \theta)}{q(z)}) \right] \quad \text{log of an expectation} \\  & \geq
\underbrace{ \sum_{z} q(z) \left[ \log \frac{p(x, z \vert \theta)}{q(z)}) \right] }_{ ELBO \space \mathcal{L}(q, \theta) } \quad \text{expectation of log, by Jensen's inequality}
\end{align}
$$

$$
\therefore \text{For any q(z), } \log p(x \vert \theta) \geq \mathcal{L}(q, \theta) 
$$

# 3. Expectation-Maximization

EM ì•Œê³ ë¦¬ì¦˜ì€ ë‹¤ìŒê³¼ ê°™ì´ ì§„í–‰ëœë‹¤.

1. íŒŒë¼ë¯¸í„° $\theta^{old}$ ë¥¼ ëžœë¤í•˜ê²Œ ì´ˆê¸°í™”í•œë‹¤.
2. $q^{\ast} = arg \space max_{q} \mathcal{L}(q, \theta^{old})$ ë¥¼ ì°¾ëŠ”ë‹¤.
3. $\theta^{new} = arg \space max_{\theta} \mathcal{L}(q*, \theta)$ ë¥¼ ì°¾ëŠ”ë‹¤.
4. ë¡œê·¸ìš°ë„ê°€ ìˆ˜ë ´í•˜ì§€ ì•Šì•˜ë‹¤ë©´ $\theta^{old} \leftarrow \theta^{new}$ ë¡œ ë†“ê³  2ë²ˆìœ¼ë¡œ ëŒì•„ê°„ë‹¤.

ì—¬ê¸°ì—ì„œ ì–¸ì œë‚˜ $\log p(x \vert \theta^{new}) \geq \log p(x \vert \theta^{old})$ ìž„ì„ ë³´ì¼ ìˆ˜ ìžˆë‹¤. **ì¦‰ ì•Œê³ ë¦¬ì¦˜ì˜ ì§„í–‰ì— ë”°ë¼ ë¡œê·¸ìš°ë„ëŠ” ë‹¨ì¡°ì¦ê°€í•œë‹¤.** ê·¸ ì´ìœ ë¥¼ ì•Œê¸° ìœ„í•´ì„œ ELBOì˜ ìˆ˜ì‹ì„ ì¡°ê¸ˆ ë‹¤ë¥¸ í˜•íƒœë¡œ ëœ¯ì–´ë³¼ í•„ìš”ê°€ ìžˆë‹¤.

## 3.1. Maximizing over $q$ for fixed $\theta = \theta^{old}$

ì•Œê³ ë¦¬ì¦˜ì˜ 2ë²ˆ ìŠ¤í…ì—ì„œëŠ” ê³ ì •ëœ $\theta^{old}$ ë¥¼ í†µí•´ ELBOë¥¼ ìµœëŒ€í™”í•˜ëŠ” PMF $q(z)$ë¥¼ ì°¾ëŠ”ë‹¤. ELBO ìˆ˜ì‹ì„ ì•½ê°„ ë³€í˜•í•˜ë©´ Negative KL Divergence ì™€ Evidenceì˜ í•©ìœ¼ë¡œ ë¶„í•´ëœë‹¤. Evidence í•­ì€ $z$ë¥¼ ì£¼ë³€í™”ì‹œí‚¨ í™•ë¥ ì´ë¯€ë¡œ $q$ê°€ ê´€ì—¬í•˜ì§€ ì•ŠëŠ”ë‹¤. ì¦‰ ìš°ë¦¬ëŠ” KL Divergenceë¥¼ ìµœì†Œí™”í•˜ëŠ” $q$ë¥¼ ì°¾ëŠ” ê²ƒì´ë‹¤. KL DivergenceëŠ” ë‘ ë¶„í¬ê°€ ê°™ì„ ë•Œ ìµœì†Œê°’ì„ ê°€ì§€ë¯€ë¡œ $q(z) = p(z \vert x, \theta^{old})$ ì´ê³  ì´ë•Œ KL Divergenceì˜ ê°’ì€ 0ì´ë¯€ë¡œ ê²°êµ­ $\mathcal{ L }(q^{\ast}, \theta^{old}) = \log p(x \vert \theta)$ ê°€ ëœë‹¤. ì•žì—ì„œ ELBOëŠ” **Evidenceì˜ í•˜í•œ**ì´ë¼ê³  í–ˆë‹¤. ë”°ë¼ì„œ $q^{\ast}$ ì—ì„œ ELBOì™€ Evidenceì˜ ê°’ì´ ê°™ì•„ì§„ë‹¤ëŠ” ê²ƒì€ $q^{\ast}$ ì—ì„œ ë‘ í•¨ìˆ˜ê°€ ì ‘í•œë‹¤ëŠ” ëœ»ì´ ëœë‹¤.

$$
\begin{align}
\mathcal{L}(q, \theta) &=
 \sum_{z} q(z) \left[ \log \frac{p(x, z \vert \theta)}{q(z)} \right] \\ &=
 \sum_{z} q(z) \left[ \log \frac{p(z \vert x, \theta) p(x \vert \theta)}{q(z)}) \right] (\text{conditional probability})\\ &=
 \sum_{z} q(z) \log \frac{p(z \vert x, \theta)}{q(z)}) + \sum_{z} q(z)\log p(x \vert \theta) \\ &=
  -KL \left[ q(z) \vert \vert p(z \vert x, \theta) \right] + \underbrace { \log p(x \vert \theta) }_{\text{no q here}} \space
\end{align}
$$


$$
\therefore q^{\ast} = arg \space max_q \mathcal{L}(q, \theta^{old}) \implies \mathcal{L(q^{\ast}, \theta^{old})} = \log p(x \vert \theta^{old})
$$


## 3.2. Maximizing over $\theta$ for fixed $q^{\ast}$


ë‹¤ìŒìœ¼ë¡œëŠ” $arg \space max_{theta} \mathcal{L}(q^{\ast}, \theta)$ ë¥¼ ì°¾ì•„ì•¼ í•œë‹¤. ELBO ìˆ˜ì‹ì„ ë³€í˜•í•˜ë©´ $\theta$ê°€ ê´€ì—¬í•˜ì§€ ì•ŠëŠ” ë‘˜ì§¸ í•­ì€ ë¬´ì‹œí•  ìˆ˜ ìžˆë‹¤. ì²«ì§¸ í•­ì€ $X$ì™€ $Y$ì˜ ê²°í•©ë¶„í¬, ì¦‰ ì™„ì „í•œ ë°ì´í„°(complete data)ì˜ ë¡œê·¸ìš°ë„ ê¸°ëŒ€ê°’ì´ë‹¤. ì¦‰ ELBOë¥¼ ìµœëŒ€í™”í•˜ëŠ” ê²ƒì€ ì™„ì „í•œ ë°ì´í„°ì˜ ë¡œê·¸ìš°ë„ ê¸°ëŒ€ê°’ì„ ìµœëŒ€í™”í•˜ëŠ” ê²ƒê³¼ ê°™ë‹¤. $q^{\ast}$ë¥¼ ì°¾ì•„ ì™„ì „í•œ ë°ì´í„°ì˜ ë¡œê·¸ìš°ë„ ê¸°ëŒ€ê°’ ìˆ˜ì‹ì„ ì„¸ìš°ëŠ” ê³¼ì •ì„ EìŠ¤í…, ì´ ê¸°ëŒ€ê°’ì„ ìµœëŒ€í™”í•˜ëŠ” $\theta^{new}$ ë¥¼ ì°¾ëŠ” 3ë²ˆ ìŠ¤í…ì„ MìŠ¤í…ì´ë¼ê³  ë¶€ë¥¸ë‹¤. ìž ìž¬ë³€ìˆ˜ $Z$ì˜ ê°’ì„ ì •í™•ížˆ ëª¨ë¥´ê¸° ë•Œë¬¸ì— ì£¼ì–´ì§„ PMF $q^{\ast}(z)$ ì—ì„œì˜ ê¸°ëŒ€ê°’ì„ ì‚¬ìš©í•œë‹¤ê³  ë³´ë©´ ë  ê²ƒ ê°™ë‹¤.


$$
\begin{align}
\mathcal{L}(q, \theta) &=
 \sum_{z} q(z) \left[ \log \frac{p(x, z \vert \theta)}{q(z)} \right] \\ &=
 \underbrace { \sum_{z} q(z) \log p(x, z \vert \theta) }_{\text{expectation of complete data log-lokelihood}} - \underbrace { \sum_{z} q(z) \log q(z) }_{\text{no theta here}} \\ &
\therefore \theta^{new} = arg \space max_\theta \sum_{z} q^{\ast}(z) \log p(x, z \vert \theta)
\end{align}
$$


## 3.3. EM Gives Monotonically Increasing Likelihood

ì§€ê¸ˆê¹Œì§€ ë…¼ì˜í•œ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ë©´ ë‹¤ìŒ ë¶€ë“±ì‹ì´ ì„±ë¦½í•œë‹¤. ì¦‰ EM ì•Œê³ ë¦¬ì¦˜ì˜ ì´í„°ë ˆì´ì…˜ì— ë”°ë¼ì„œ ë¡œê·¸ìš°ë„ê°€ ë‹¨ì¡°ì¦ê°€í•¨ì„ ë³´ì¼ ìˆ˜ ìžˆë‹¤.


$$
\begin{align}
\log p(x \vert \theta^{new}) &\geq   \mathcal{L}(q^{\ast}, \theta^{new}) \quad \mathcal{L}\text{ is a lower bound of evidence} \\ &\geq
\mathcal{L}(q^{\ast}, \theta^{old})\quad \text{By definition of } \theta^{new} \\ &=
\log p(x \vert \theta^{old}) \quad \text{Lower bound of evidence is tight at } q^{\ast}
\end{align}
$$


## 3.4. Global Maximum

ë˜í•œ ELBOì˜ ê¸€ë¡œë²Œ ë§¥ì‹œë©ˆ $\theta^\ast$ì´ ë¡œê·¸ìš°ë„ì˜ ê¸€ë¡œë²Œ ë§¥ì‹œë©ˆê³¼ ê°™ë‹¤ëŠ” ì‚¬ì‹¤ì„ ë³´ì¼ ìˆ˜ ìžˆë‹¤. ELBOì˜ ê¸€ë¡œë²Œ ë§¥ì‹œë©ˆì„ $\mathcal{L}(q^{\ast}, \theta^\ast)$ ë¼ í•˜ìž. ì „ì—­ ìµœëŒ€ì´ë¯€ë¡œ ë‹¤ìŒì´ ì„±ë¦½í•œë‹¤. ìœ„ì—ì„œëŠ”  $q$ë¥¼ ì—…ë°ì´íŠ¸í•  ë•Œ $q^{\ast}(z) = p(z \vert x, \theta^{old})$ ë¡œ ì¼ì§€ë§Œ ê¸€ë¡œë²Œ ë§¥ì‹œë©ˆì—ì„œëŠ” ê²°êµ­ $\theta$ê°€ ê³ ì •ë˜ë¯€ë¡œ $q^{\ast}(z) = p(z \vert x, \theta^\ast)$ ê°€ ëœë‹¤.


$$\mathcal{L}(q^{\ast}, \theta^\ast) \geq \mathcal{L}(q, \theta), \space \forall (q, \theta) \quad where \quad q^{\ast}(z) = p(z \vert x, \theta^\ast) $$


ì´ ë•Œ $\theta^\ast$ì€  ELBO ë¿ë§Œ ì•„ë‹ˆë¼ Evidenceì˜ ì „ì—­ ìµœëŒ€ì ì´ê¸°ë„ í•˜ë‹¤. ìž„ì˜ì˜ íŒŒë¼ë¯¸í„° $\thetaâ€²$ì™€ ì´ì— ëŒ€í•´ $qâ€²(z) = p(z \vert x, \thetaâ€²)$ ì„ ë§Œì¡±í•˜ëŠ” $qâ€²$ ë¥¼ ê°€ì •í•˜ìž. ì´ ê²½ìš° Lower boundê°€ ì£¼ì–´ì§„ $\thetaâ€²$ ì— ëŒ€í•´ ìµœëŒ€í™”ëœ ìƒíƒœë¡œ ë” ì´ìƒ ì¦ê°€í•  ìˆ˜ ì—†ìœ¼ë©° Evidenceì™€ ì ‘í•´ ìžˆë‹¤. ì¦‰ ë‘˜ì€ ê°™ì€ ê°’ì„ ê°€ì§„ë‹¤. ì´í›„ ì „ì—­ ìµœëŒ€ì ì— ëŒ€í•´ì„œ $\mathcal{L}(q^{\ast}, \theta^\ast) \geq \mathcal{L}(q^âˆ—, \theta^âˆ—)$ ê°€ ì„±ë¦½í•œë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ $\mathcal{L}(q^âˆ—, \theta^âˆ—)$ ëŠ” íƒ€ì´íŠ¸í•œ ELBO ì´ë¯€ë¡œ Evidenceì™€ ê°™ë‹¤.


$$
\begin{align}
\log p(x|\thetaâ€²) &= \mathcal{L}(qâ€², \thetaâ€²) + \underbrace { KL[qâ€²,p(z|x, \thetaâ€²)] }_{\text{equals to 0}} \\ &=
\mathcal{L}(qâ€²,\thetaâ€²) \\ & \leq
\mathcal{L}(q^âˆ—, \theta^âˆ—) \quad \text{Global maximum} \\ &=
\log p(x \vert \theta^âˆ—) \quad \text{Lower bound is tight}
\end{align} \\
$$



# 4. Mixture of Gaussians

ë§ˆì§€ë§‰ìœ¼ë¡œ, ìœ„ì—ì„œ ê°€ìš°ì‹œì•ˆ í˜¼í•© ëª¨í˜•ì˜ ëª¨ìˆ˜ë¥¼ ì‹¤ì œë¡œ ì¶”ë¡ í•´ë³´ìž. ê° í´ëŸ¬ìŠ¤í„°ì˜ ë¹„ìœ¨ê³¼ ëª¨ìˆ˜ë¥¼ ë‚˜ë¦„ëŒ€ë¡œ ê°€ê¹ê²Œ ì¶”ë¡ í•´ë‚´ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìžˆë‹¤. ELBOì˜ ì „ì—­ ìµœëŒ€ê°€ ê²°êµ­ì€ ë¡œê·¸ìš°ë„ì˜ ì „ì—­ ìµœëŒ€ì™€ ê°™ê¸°ëŠ” í•˜ì§€ë§Œ, ì•Œê³ ë¦¬ì¦˜ì´ í•­ìƒ ì „ì—­ ìµœëŒ€ë¥¼ ì°¾ì•„ê°ˆ ìˆ˜ ìžˆëŠ”ê°€ëŠ” ë‹¤ë¥¸ ë¬¸ì œì´ë‹¤. EM ì•Œê³ ë¦¬ì¦˜ì€ ìˆ˜ë ´ì´ ë³´ìž¥ë˜ì§€ë§Œ ê·¸ê²ƒì´ ì „ì—­ ìµœëŒ€ë¼ëŠ” ë³´ìž¥ì´ ì—†ë‹¤. ì¦‰ ì´ˆê¸°ê°’ì„ ì–´ë–»ê²Œ ì„¤ì •í•˜ëŠ”ì§€ì— ë”°ë¼ ì•Œê³ ë¦¬ì¦˜ì˜ ìˆ˜ë ´ ê²°ê³¼ê°€ ë‹¬ë¼ì§ˆ ìˆ˜ ìžˆë‹¤. ì°¸ê³ í•œ ì•¤ë“œë¥˜ ì‘ êµìˆ˜ì˜ ê°•ì˜ì—ì„œëŠ” K-Means ì•Œê³ ë¦¬ì¦˜ì„ ì˜ˆì‹œë„ ëžœë¤ ì„¼íŠ¸ë¡œì´ë“œì—ì„œ ì‹œìž‘í•˜ê¸°ë³´ë‹¤ ë°ì´í„°ì—ì„œ ì´ˆê¸°ê°’ì„ ë½‘ì•„ë‚´ëŠ” ê²ƒì„ ì¶”ì²œí•œë‹¤. ì´ ì½”ë“œì—ì„œë„ ìœ ì‚¬í•œ ë°©ë²•ì„ ë”°ëžë‹¤.


```python
np.random.seed(2021)

x1 = np.random.normal(1, 1, 100)
x2 = np.random.normal(10, 3, 50)
x = np.append(x1, x2)
np.random.shuffle(x)
x = x.reshape((-1, 1))
```


```python
# ì •ê·œë¶„í¬ì˜ pdf
def pdf(x, mean, std):
    return np.exp(-((x-mean)**2)/(2 * (std**2))) / (std * (2 * np.pi)**0.5)
```


```python
# Evidence ì˜ ë¡œê·¸ìš°ë„
def log_likelihood(x, mu, sigma, p):
    return np.log((pdf(x, mu, sigma) * p).sum(axis=1)).sum().round(4)
```

## 4.1. Choose initial $\theta^{old}$


```python
p = np.array([[0.5, 0.5]], dtype=np.float64)
mu = np.random.choice(x.flatten(), 2, replace=True)[np.newaxis, :]
sigma = np.array([[np.std(x), np.std(x)]], dtype=np.float64)
logL = log_likelihood(x, mu, sigma, p).round(4)
eps = 1e-6
```

## 4.2. Expectation - Maximization


```python
density = pdf(x, mu, sigma)
dp = density * p
pz_x = (dp / dp.sum(axis=1, keepdims=True)).sum(axis=0, keepdims=True)
```


```python
row_format = "{:^15}|{:^15}|{:^15}|{:^15}|{:^15}|{:^15}|{:^15}|{:^15}"
print(row_format.format("iteration", "log-likelihood", "P(Z=0)", "P(Z=1)", "mu_1", "mu_2", "sigma_1", "sigma_2"))
print(row_format.format("=" * 15, "=" * 15, "=" * 15, "=" * 15, "=" * 15, "=" * 15, "=" * 15, "=" * 15))


for i in range(100):
    
    density = pdf(x, mu, sigma)
    dp = density * p
    pz_x = (dp / dp.sum(axis=1, keepdims=True))
    
    p = pz_x.mean(axis=0)[np.newaxis, :].round(4)
    sigma = np.sqrt((pz_x * (x - mu)**2).sum(axis=0) / pz_x.sum(axis=0))[np.newaxis, :].round(4)
    mu = ((x * pz_x).sum(axis=0) / pz_x.sum(axis=0))[np.newaxis, :].round(4)
    
    logL_new = log_likelihood(x, mu, sigma, p).round(4)
    gain = logL_new - logL
    
    assert gain >= 0
    
    print(row_format.format(i+1, logL, p[0][0], p[0][1], mu[0][0], mu[0][1], sigma[0][0], sigma[0][1]))
    
    if gain < eps:
        
        
        print(row_format.format("=" * 15, "=" * 15, "=" * 15, "=" * 15, "=" * 15, "=" * 15, "=" * 15, "=" * 15))
        print(row_format.format("-", logL, p[0][0], p[0][1], mu[0][0], mu[0][1], sigma[0][0], sigma[0][1]))
        
        print("\nAlgorithm Converged!")
        
        break
    
    logL = logL_new
```

```
       iteration   |log-likelihood |    P(Z=0)     |    P(Z=1)     |     mu_1      |     mu_2      |    sigma_1    |    sigma_2    
    ===============|===============|===============|===============|===============|===============|===============|===============
           1       |   -459.966    |    0.5786     |    0.4214     |    1.7004     |    8.0079     |    2.6411     |    5.1511     
           2       |   -418.9214   |    0.5904     |    0.4096     |    1.3054     |     8.759     |    1.5495     |    4.7224     
           3       |   -382.1583   |    0.6168     |    0.3832     |     1.102     |    9.5997     |    1.0135     |    4.0415     
           4       |   -360.4603   |    0.6368     |    0.3632     |    1.0722     |    10.1216    |    0.9333     |     3.429     
           5       |   -356.5254   |    0.6489     |    0.3511     |    1.0748     |    10.4287    |    0.9394     |    3.0157     
           6       |   -354.8216   |    0.6553     |    0.3447     |    1.0845     |    10.5818    |    0.9495     |    2.8023     
           7       |   -354.3142   |    0.6576     |    0.3424     |    1.0901     |    10.6362    |    0.9549     |    2.7281     
           8       |   -354.2459   |    0.6583     |    0.3417     |    1.0921     |    10.6517    |     0.957     |     2.708     
           9       |   -354.2402   |    0.6585     |    0.3415     |    1.0926     |    10.6558    |    0.9576     |    2.7028     
          10       |   -354.2398   |    0.6585     |    0.3415     |    1.0928     |    10.6569    |    0.9578     |    2.7015     
    ===============|===============|===============|===============|===============|===============|===============|===============
           -       |   -354.2398   |    0.6585     |    0.3415     |    1.0928     |    10.6569    |    0.9578     |    2.7015     
    
    Algorithm Converged!
```

## ì°¸ê³ ìžë£Œ

- [Lecture 14 - Expectation-Maximization Algorithms \| Stanford CS229: Machine Learning (Autumn 2018)](https://www.youtube.com/watch?v=rVfZHWTwXSA&t=2192s)
- [27. EM Algorithm for Latent Variable Models](https://www.youtube.com/watch?v=lMShR1vjbUo)
