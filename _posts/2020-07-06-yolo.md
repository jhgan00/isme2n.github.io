---
layout: post
title: "[ML] YOLO v1 논문 정리"
categories: doc
tags: [ml]
comments: true
---

> You Only Look Once: Unified, Real-Time Object Detection

## 1. Introduction

Yolo의 컨셉은 object detection 문제를 multi-output regression으로 풀자는 것이다. 기존의 방법들은 sliding window나 regional proposal methods 등을 통해 바운딩 박스를 잡은 후 탐지된 바운딩 박스에 대해 분류를 수행한다. 하지만 이렇게 복잡한 파이프라인을 구성하면 당연히 학습과 예측이 느려지고 최적화도 어려워진다. 이에 비해 YOLO는 하나의 컨볼루션 네트워크를 통해 대상의 위치와 클래스를 한번에 예측한다. 저자들은 YOLO의 강점을 크게 세 가지로 요약힌다. **첫째, 학습 파이프라인이 기존의 detection 모델들에 비해 간단하기 때문에 학습과 예측의 속도가 매우 빠르다. 둘째, 모든 학습 과정이 이미지 전체를 통해서 일어나기 때문에 단일 대상의 특징뿐 아니라 이미지 전체의 맥락을 학습하게 된다. 셋째, 대상의 일반적인 특징을 학습하기 때문에 다른 영역으로의 확장에서도 뛰어난 성능을 보인다.** YOLO는 빠른 속도를 자랑하지만 작은 물체들의 위치를 잘 탐지하지 못한다. 여기에는 일종의 trade-off 관계가 존재한다.

- DPM: sliding window approach + classification
- R-CNN: regional proposal methods + classification

## 2. Unified Detection

![Conf score of bounding boxes & Class prob of grid cells](/assets/img/yolov1-1.png)

> **Figure 2: The Model.** Out system models detection as as regression problem. It divides the image into an $$S \times S$$ grid and for each grid cell predicts $$B$$ bounding boxes, confidence for those boxes, and $$C$$ class probalities. These predictions ard encoded as an $$S \times S \times (B * 5 + C)$$ tensor.

**YOLO는 하나의 이미지를 입력받아서 $$(S,S,B \times 5 + C)$$ 사이즈의 출력을 내놓는다.** $$S, \space B, \space C$$가 각각 무엇을 의미하는지, 어떤 과정을 거쳐서 이러한 출력이 만들어지는지 천천히 살펴보자. YOLO는 입력된 이미지를 $$S$$개의 그리드로 나눈다. **Figure 2의 가장 왼쪽 사진이 바로 이미지를 $$S \times S$$개의 그리드로 구분한 상태이다.**

**하나의 그리드 셀은 B개의 바운딩 박스를 예측한다.** 즉 하나의 그리드 셀은 최대 $$B$$개의 객체를 탐지할 수 있다. **바운딩 박스는 다시 $$x$$, $$y$$, $$w$$, $$h$$, $$confidence$$의 5개 예측치로 구성된다.** $$(x,y)$$는 바운딩 박스의 중심 좌표를 나타낸다. 좌표는 그리드 셀의 경계에 대해 표현되어 0 ~ 1 사이의 값을 갖는다. $$(w,h)$$는 바운딩 박스의 너비와 높이를 나타내는 값으로 전체 이미지에 대해 표현되어 역시 0 ~ 1의 사이의 값을 갖는다. 즉 바운딩 박스의 중심은 항상 그리드 셀 안에 존재하지만 박스의 크기는 그리드 셀보다 커질 수 있다. 

$$confidence = Pr(Object) \times IOU_{pred}^{truth}$$와 같이 정의한다. `confidence` 의 정답은 실제 바운딩 박스와 예측된 바운딩 박스의 IOU 값으로 정해진다. 즉 정답을 결정하는데 예측치가 관여한다. 논문에서는 모델이 내놓는 `confidence` 점수를 Pr(Object) x IOU로 정의한다. 예측된 박스 안에 객체가 존재하지 않는다면, 이상적인 모델에서는 Pr(Object)가 0이 되어  `confidence` 역시 0이 되어야 한다. 반대로 예측된 박스 안에 객체가 존재한다면, 이상적인 모델에서는 Pr(Object)가 1이 되어  `confidence` 는 IOU와 같아져야 한다. 이러한 성질로 인해서 연구자들은 `confidence` 점수와 IOU를 예측치-정답의 쌍처럼 사용할 수 있다고 판단한 듯 하다. **Figure 2의 가운데 위쪽 사진이 각 그리드 셀에서 바운딩 박스를 예측한 결과이다.**

**그리드 셀은 물체의 클래스에 대한 $$C$$개의 조건부 확률, $$Pr(Class_i \| Object)$$를 예측한다.** 그리드 셀 안의 바운딩 박스의 개수 $$B$$와는 무관하게 반드시 $$C$$개의 예측치를 내놓는다는 점에 주의한다. Figure 2의 가운데 아래쪽 사진이 각 그리드 셀에서 바운딩 박스를 예측한 결과이다

따라서 모델의 output은 아래 그림처럼 표현된다. 논문에서는 Pascal VOC 데이터셋에 대해 $$S=7, \space B=2$$를 사용했고 클래스의 개수 $$C=20$$이므로 최종 예측은 $$7 \times 7 \times 30$$의 텐서가 된다.

![](https://miro.medium.com/max/700/1*YG6heD55fEmZeUKRSlsqlA.png)

테스트 시점에서는 그리드 셀의 클래스 확률을 바운딩 박스의 confidence score와 곱해서 바운딩 박스의 class-specific confidence score를 산출한다. 이 결과에 Non-maximal surpress를 적용하여 **Figure 2의 가장 오른쪽 그림과 같은 최종 detection 결과를 얻는다.**

$$Pr(Class_i|Object) \times Pr(Object) \times IOU^{truth}_{pred} = Pr(Class_i) \times IOU^{truth}_{pred}$$

### 2.1. Network design

![](/assets/img/yolov1-2.png)

> **Figure 3: The Architecture.** Our detection network has 24 convolutional layers followed by 2 fully connected layers. Alternating $$1 \times 1$$ convolutional layers reduce the features space from preceding layers. We pretain the convolutional layers on the ImageNet classification task at half the resolution($$224 \times 224$$ input image) and then double the resolution for detection.

YOLO는 GoogLeNet의 구조를 차용해 24개의 컨볼루션 레이어와 2개의 FC 레이어를 사용한다. 다만 인셉션 모듈 대신에 $$1 \times 1$$ reduction layer만을 사용한다. FastYOLO는 9개의 컨볼루션 레이어만을 사용한다. 인풋으로는 $$448 \times 448$$의 이미지를 받아 $$7 \times 7 \times 30$$의 텐서를 출력한다.

### 2.2. Training

Detection 학습 전에 $$(224 \times 224)$$ 크기의 이미지넷 데이터를 통해 분류를 먼저 학습한다. Pretraining 과정에서는 20개의 컨볼루션 레이어에 average pooling, FC 레이어를 붙여 네트워크를 구성한다. Detection 훈련 단계에서는 20개의 컨볼루션 레이어 뒤에 네 개의 컨볼루션 레이어와 두 개의 FC 레이어를 추가해서 최종 네트워크를 구성한다. 마지막 레이어에서는 선형 출력을 사용하고 나머지 레이어에서는 $$leak=0.1$$의 leaky relu를 사용한다. 입력 이미지의 해상도는 $$448 \times 448$$로 키워준다. 마지막 레이어는 클래스의 확률과 바운딩 박스의 위치 및 크기를 한 번에 예측한다. 위에서 언급한대로 바운딩 박스의 위치와 크기는 각각 그리드 셀과 전체 이미지에 대해서 0 ~ 1로 정규화한다.

![](https://lilianweng.github.io/lil-log/assets/images/yolo-responsible-predictor.png)

> 그림 출처: Lilian Weng, [Object Detection Part 4: Fast Detection Models](https://lilianweng.github.io/lil-log/2018/12/27/object-detection-part-4.html)

YOLO의 손실 함수에 대해 이야기하기 전에 먼저 responsible이라는 개념을 짚고 넘어가야 한다. Responsible이라는 개념은 하나의 그리드 셀 안에서는 하나의 물체에 대해 하나의 바운딩 박스를 대응시키기 위한 것이다. 위 그림에서처럼 그리드 셀 $$i$$가 파란색으로 표현된 두 개의 바운딩 박스를 예측했다고 가정해보자. 두 개의 박스는 모두 빨간색의 물체를 탐지했다고 볼 수 있지만 IOU에는 차이가 있다. 이 경우 더 큰 IOU를 갖는 바운딩 박스를 해당 객체에 대해 responsible하다고 하며, 로스는 responsible한 바운딩 박스에 대해서만 계산된다. 논문의 로스 수식에서는 responsible 여부를 $$1_{i,j}^{obj}$$로 나타낸다.  

> (p.2) If the center of an object falls into a grid cell, that grid cell is responsible for detecting that object.

> (p.3) At training time we only want one bounding box predictor to be responsible for each object. We assign one predictor to be “responsible” for predicting an object based on which prediction has the highest current IOU with the ground truth.


사실 위 인용을 보면 "responsible" 이라는 개념은 그리드 셀에 대해서도, 바운딩 박스에 대해서도 사용될 수 있다. 하지만 로스의 관점에서 "responsible" 은 결국 바운딩 박스에 대한 개념으로 생각하는게 편하다. 즉 하나의 물체에 대한 예측을 책임지는 유일한 바운딩 박스를 결정하는 문제이다. 아래 수식을 보면 예측에 대해 "responsible" 하지 않은 박스의 좌표 크기 로스 텀은 0이 되는 것을 알 수 있다. 즉 책임이 없는 박스들에 대해서는 좌표와 크기의 로스를 면제해주자는 아이디어이다.

먼저 그리드 셀의 단위에서는 해당 셀이 어떤 물체의 중심을 포함하는 경우 그 물체의 탐지에 대해 "responsible"하다고 한다. 하지만 YOLO는 모델의 구조상 하나의 그리드 셀은 여러 개의 바운딩 박스를 예측한다. 논문예써는 하나의 물체에 대해 정확히 하나의 바운딩 박스를 대응시키기 위해 바운딩 박스에 대해서도 "responsible" 의 조건을 정의한다. 즉 물체에 대해 가장 높은 IOU를 갖는 박스를 물체에 대해 "responsible" 하다고 정한다. 정리하면, 어떤 물체의 중심을 포함하는 셀에서 예측된 바운딩 박스 중 실제 바운딩 박스와 가장 큰 IOU를 갖는 박스를 그 물체에 대해 "responsible" 하다고 말한다. 

$$
\begin{aligned}
\lambda_{coord} \sum_{i=0}^{S^2} \sum_{j=0}^{B} 1_{i,j}^{obj}[(x_i - \hat{x_i})^2 + (y_i - \hat{y_i})^2]\\
+ \space \lambda_{coord} \sum_{i=0}^{S^2} \sum_{j=0}^{B} 1_{i,j}^{obj}[(\sqrt{w_i} - \sqrt{\hat{w_i}})^2 + (\sqrt{h_i} - \sqrt{\hat{h_i}})^2]\\
+ \space \sum_{i=0}^{S^2} \sum_{j=0}^{B} 1_{i,j}^{obj}(C_i - \hat{C_i})^2\\
+ \space \lambda_{noobj}\sum_{i=0}^{S^2} \sum_{j=0}^{B} 1_{i,j}^{noobj}(C_i - \hat{C_i})^2\\
+ \space \sum_{i=0}^{S^2} 1_{i}^{obj} \sum_{c \in classes} (p_i(c) - \hat{p_i}(c)) 
\end{aligned}
$$


YOLO의 손실 함수는 기본적으로 SSE에 기반한다. 하지만 SSE를 그대로 사용하면 몇 가지 문제가 있다. SSE를 그대로 사용할 경우 LOC, CLS 에러가 동등하게 취급되며, 모든 이미지에는 물체를 포함하지 않는 그리드가 훨씬 많기 때문에 confidence score는 0으로 수렴할 가능성이 크다. 이를 조정하기 위해 YOLO는 $$\lambda_{coord}, \lambda_{noobj}$$라는 파라미터를 도입해서 로스를 조정한다. 논문에서는 $$\lambda_{coord}=5$$로 설정하여 바운딩 박스의 좌표에 대한 로스를 증가시키고, $$\lambda_{noobj}=0.5$$로 설정하여 물체를 포함하지 않는 바운딩 박스의 confidence score 로스를 감소시킨다.

첫 항은 바운딩 박스의 좌표에 대한 로스이다. 좌표의 로스는 바운딩 박스가 어떤 객체에 대해 responsible한 경우에만 유효하며, $$\lambda_{coord}$$ 항을 통해 로스를 조정해준다. 둘째 항은 바운딩 박스의 크기에 대한 로스이다. 역시 responsible한 박스에 대해사만 유효하며, $$\lambda_{coord}$$항을 통해서 조정된다. 셋째, 넷째 항은 바운딩 박스의 confidence에 대한 로스이다. 셀 안에 객체가 존재하지 않는 케이스들로 인해 confidence가 0으로 수렴하는 현상을 방지하지 위해서 박스가 responsible하지 않은 경우에는 $$\lambda_{noobj}$$를 곱해 로스를 줄여준다. 마지막 항은 각 셀의 클래스 확률에 대한 로스이다. $$1_{i}^{obj}$$는 그리드 셀 $$i$$ 안에 객체가 존재하는지의 여부를 나타낸다. 즉 클래스에 대한 로스는 셀 안에 객체가 존재하는 경우에만 유효하다.

### 2.3. Inference

네트워크의 구조에 따라 모델은 하나의 이미지에 대해 $$7 \times 7 \times 2 = 98$$개의 바운딩 박스와 클래스 확률을 출력한다. 물체가 정확히 특정 셀에 맞아 떨어질 수도 있지만, 크기가 큰 물체나 여러 셀의 경계 근처에 위치하는 물체는 여러 개의 셀에 의해서 포착될 수 있다. 이러한 경우에는 non-maximal surpression을 적용해서 중복을 제거한다. 

### 2.4. Limitations of YOLO

- 하나의 그리드 셀은 두 개의 바운딩 박스만을 예측한다는 제약으로 인해 가까이에 존재하는 여러 개의 객체를 잘 탐지하지 못한다.

- 주어진 데이터를 통해서 바운딩 박스를 학습하기 때문에 같은 물체라도 새로운 aspect ratio로 주어지는 경우에는 잘 탐지하지 못한다. 

- 모델이 여러 개의 다운샘플링 레이어를 사용하기 때문에 상대적으로 거친 피쳐들을 학습하게 된다.

- YOLO는 다른 모델들에 비해 상대적으로 큰 localization error를 갖는다. 저자들은 이러한 현상의 원인을 손실함수의 구조에서 찾는 듯 하다. YOLO의 손실 함수는 크기가 큰 물체에서의 오차와 크기가 작은 물체에서의 오차를 동등하게 취급한다. 하지만 같은 크기의 오차는 작은 물체의 IOU에 대해 상대적으로 큰 영향을 미친다. 즉 작은 물체들의 IOU는 로스의 크기에 비해 상대적으로 작을 것이고, 이는 localization error로 이어지는 것이다.

## 참고자료

- Redmon, J., Divvala, S., Girshick, R., Farhadi, A.: [You only look once: unified, real-time object detection](https://arxiv.org/abs/1506.02640). In: CVPR (2016)