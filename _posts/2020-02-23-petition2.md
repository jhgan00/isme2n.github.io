---
layout: post
title: "[국민청원] 청와대 국민청원 Word2Vec 모델링"
categories: [doc, project]
tags: petition
comments: true
---

청와대 국민청원 게시판에 올라온 글들을 대상으로 Word2Vec 모델링을 하는 과정입니다. 기본적으로 '정의'와 높은 코사인 유사도를 갖는 단어들을 검출하기 위해서 Word2Vec 모델링을 진행하였습니다. 제가 프로젝트에 합류한 시점에는 이미 데이터 크롤링이 전부 완료된 상태였기 때문에 간단한 전처리 이후 즉시 모델링을 진행할 수 있었습니다. GCP에 아래와 같은 사양의 머신을 구성하여 코드를 실행하였습니다. 명사 추출에 `Mecab`을 사용하였기 때문에, 윈도우 환경에서는 일부 코드가 실행 불가능할 수 있습니다.

>- Ubuntu 18.04 LTS
- 52GB RAM,
- Cores

```python
import pandas as pd
import numpy as np
import re
import os

from konlpy.tag import Mecab, Kkma
from gensim.models import Word2Vec
from gensim.test.utils import datapath
```

## 1. 데이터 전처리 

가장 먼저 중복된 게시물 제거, 불필요한 컬럼 제거, 한글 및 띄어쓰기만 남기기 등의 기본적인 전처리를 진행해줍니다. 이후 문장 단위로 모델을 훈련할 예정이므로 어느 정도 유의미한 분석이 가능한 문서들만을 남겨줍니다. 기준은 30글자 이상, 5어절 이상으로 잡았습니다. 최종적으로 약 38만 건의 문서를 대상으로 모델링했습니다.

```python
# 중복 제거
data = pd.read_csv("./data/data.csv").dropna().drop_duplicates("Content") # 중복 제거

# 카테고리 정렬, 불필요한 컬럼 드랍, 한글 및 띄어쓰기만 남겨주기
hangul = re.compile('[^ ㄱ-ㅣ가-힣]+') # 한글과 띄어쓰기를 제외한 모든 글자 삭제
data = data.assign(
    Content = data.Content.apply(lambda x: x.strip()[1:-1]).apply(lambda x: hangul.sub("", x).strip()),
    Category = data.Category.apply(lambda x: x.strip()).replace({
        "외교/통일/국토":"외교/통일/국방", "안전/환제":"안전/환경", "보건복업지":"보건복지"
    })).drop(columns = ["Unnamed: 0", "Unnamed: 0.1"]).dropna() # 간단한 전처리

# 문장 단위로 훈련할 것이므로 유의미하게 분석 가능한 수준의 문서만 남김
data = data[(data.Content.apply(len) >= 30) & (data.Content.apply(lambda x: len(x.split(" ")) >= 5))] # 30글자 5어절 이상만

# 데이터 저장
data.to_csv("data/data.csv", index=False, encoding = "utf-8")
```

## 2. 모델 훈련

문장 토크나이징에는 `Kkma`, 명사 추출에는 `Mecab` 클래스를 사용하였습니다. 인코딩 등의 문제가 가끔 발생하므로 `Kkma` 클래스에 예외 처리 기능을 추가하여 `myKkma` 클래스를 정의해주었습니다.

```python
class myKkma(Kkma):
     def sentences(self, phrase):
        """
        Sentence detection: 예외처리 추가
        """
        try:
            sentences = self.jki.morphAnalyzer(phrase)
            if not sentences: return []
            return [sentences.get(i).getSentence() for i in range(sentences.size())]
        except:
            return []

kkma, mecab = myKkma(), Mecab()
```
카테고리별로 정의와 관련을 맺는 단어들에 차이가 있을 수 있다고 판단하여, 각 카테고리별로 다른 모델을 훈련하고 그 결과를 저장하는 `modelTrain` 함수를 정의하였습니다. 카테고리마다 문장 단위로 전처리한 데이터들을 데이터 디렉토리에 저장해줍니다. 다음으로 모델을 불러와 정의와 가장 유사한 50개 단어를 반환하는 `loadAndVal` 함수를 정의합니다.

```python
def modelTrain(cat):
    df = data[data.Category == cat] # 카테고리 필터
    filename = cat.replace("/","")
    result = df.Content.apply(lambda x: "|".join(kkma.sentences(x))) # 문장 단위로 구분
    result.reset_index().drop("index", axis=1).to_csv(f"data/{filename}.csv", index=False) # 문장 파싱 결과 저장
    tmp = pd.read_csv(f"data/{filename}.csv").dropna()
    nouns = [mecab.nouns(sentence) for sentence in tmp.Content.apply(lambda x: x.split("|")).sum()] # 명사 추출
    model = Word2Vec(nouns, window=3, min_count=3, size=100, sg=1) # 모델 훈련
    model.save(f"models/skipgram/{filename}.model") # 모델 저장

def loadAndVal(path, topn=50):
    mostSimilar = Word2Vec.load(path).wv.most_similar("정의", topn=topn)
    
    result = pd.DataFrame({
        "category":path.split("/")[-1].split(".")[0],
        "rank":np.arange(1,51),
        "keyword":[word for word, distance in mostSimilar],
        "distance":[distance for word, distance in mostSimilar],
    })
    
    return result
```

이제 실제로 모델을 훈련하고, 결과를 피봇 테이블로 변환하여 확인해줍니다. 나름대로 흥미로운 결과들이 관찰됩니다. 카테고리마다 유사하게 나타나는 어휘들도 있고, 각 카테고리에 특수한 어휘들도 눈에 띕니다.

```python
sgModels = [f"./models/skipgram/{filename}" for filename in os.listdir("./models/skipgram/")]
sgResult = pd.concat(list(pd.Series(sgModels).apply(loadAndVal)))
print(sgResult.pivot(index = "rank", columns = "category", values = "keyword").head(10))
```

```
   rank 경제민주화 교통건축국토    기타 농산어촌  ... 육아교육 인권성평등  일자리 저출산고령화대책  정치개혁    행정
0     1    구현     구현    구현   촛불  ...   만인    구현   평등       평화    공의    구현
1     2    구호     만인    법치   인권  ...  출발점    공의   공정      자국민    구현    공의
2     3    만인     공평    갈망   주권  ...   구현   법질서   구현       공정    공정    공정
3     4    공서     반칙    공의   민주  ...   수호    첩경  정당화       애국  준법정신    반칙
4     5    공의     공의   법질서   평등  ...   주창   솔로몬   함의       민주   여신상  좌지우지
5     6    승승     박수  법치주의   탄생  ...   표방   여신상  주권자       부강   사제단    배신
6     7    반칙     추상   여신상   응원  ...   평등    실현  다수결       잡종  법치주의    근간
7     8    불의    취임사   신의칙   권력  ...   승복   법치국   공평       후손   수호자  법치주의
8     9    실천     특권    불의   역사  ...   대한    순리  취임사       서양  정의로운   여신상
9    10    정화     저것    련지  자존심  ...   공의    응징   용단      구성원    법치    불의
```

마지막으로 카테고리별 데이터들을 모두 불러와 전체 문서를 대상으로 모델링을 진행한 후, 기존 결과와 병합해서 저장해줍니다.

```python
filelist = os.listdir("./data"); filelist.remove("data.csv"); # 문장 단위로 잘라놓은 데이터들
sentences = pd.concat([pd.read_csv(f"./data/{filename}") for filename in filelist]).dropna() # 읽어와서 합치기

result = []
for i in range(38):
    start = i * 10000; end = start + 10000
    tmp = sentences.iloc[start:end].Content.apply(lambda x: x.split("|"))
    tmp = list(map(mecab.nouns,tmp.sum()))
    result.append(tmp)
    print(i)

nouns = np.array(result).sum()

model = Word2Vec(nouns, window=3, min_count=3, size=100, sg=1) # 모델 훈련
model.save("./models/skipgram/total.model")

keywords = loadAndVal("./models/skipgram/total.model")
sgResult = pd.read_csv("./정의단어/정의단어_문장단위.csv")
pd.concat([sgResult, keywords]).to_csv("./정의단어/정의단어_문장단위.csv", index=False)
pd.concat([sgResult, keywords]).pivot(index = "rank", columns = "category", values = "keyword").to_csv("./정의단어/피봇테이블_문장단위.csv")
```