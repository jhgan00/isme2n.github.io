---
layout: post
title: "[국민청원] 청와대 국민청원 Word2Vec 모델링"
categories: [doc, project]
tags: petition
comments: true
---

청와대 국민청원 게시판에 올라온 글들을 대상으로 Word2Vec 모델링을 하는 과정입니다. 기본적으로 '정의'와 높은 코사인 유사도를 갖는 단어들을 검출하기 위해서 Word2Vec 모델링을 진행하였습니다. 제가 프로젝트에 합류한 시점에는 이미 데이터 크롤링이 전부 완료된 상태였기 때문에 간단한 전처리 이후 즉시 모델링을 진행할 수 있었습니다. GCP에 아래와 같은 사양의 머신을 구성하여 코드를 실행하였습니다. 명사 추출에 `Mecab`을 사용하였기 때문에, 윈도우 환경에서는 일부 코드가 실행 불가능할 수 있습니다.

>- Ubuntu 18.04 LTS
- 52GB RAM,
- Cores

```python
import pandas as pd
import numpy as np
import re
import os

from konlpy.tag import Mecab, Kkma
from gensim.models import Word2Vec
from gensim.test.utils import datapath
```

## 1. 데이터 전처리 

가장 먼저 중복된 게시물 제거, 불필요한 컬럼 제거, 한글 및 띄어쓰기만 남기기 등의 기본적인 전처리를 진행해줍니다. 이후 문장 단위로 모델을 훈련할 예정이므로 어느 정도 유의미한 분석이 가능한 문서들만을 남겨줍니다. 기준은 30글자 이상, 5어절 이상으로 잡았습니다. 최종적으로 약 38만 건의 문서를 대상으로 모델링했습니다.

```python
# 중복 제거
data = pd.read_csv("./data/data.csv").dropna().drop_duplicates("Content") # 중복 제거

# 카테고리 정렬, 불필요한 컬럼 드랍, 한글 및 띄어쓰기만 남겨주기
hangul = re.compile('[^ ㄱ-ㅣ가-힣]+') # 한글과 띄어쓰기를 제외한 모든 글자 삭제
data = data.assign(
    Content = data.Content.apply(lambda x: x.strip()[1:-1]).apply(lambda x: hangul.sub("", x).strip()),
    Category = data.Category.apply(lambda x: x.strip()).replace({
        "외교/통일/국토":"외교/통일/국방", "안전/환제":"안전/환경", "보건복업지":"보건복지"
    })).drop(columns = ["Unnamed: 0", "Unnamed: 0.1"]).dropna() # 간단한 전처리

# 문장 단위로 훈련할 것이므로 유의미하게 분석 가능한 수준의 문서만 남김
data = data[(data.Content.apply(len) >= 30) & (data.Content.apply(lambda x: len(x.split(" ")) >= 5))] # 30글자 5어절 이상만

# 데이터 저장
data.to_csv("data/data.csv", index=False, encoding = "utf-8")
```

## 2. 모델 훈련

문장 토크나이징에는 `Kkma`, 명사 추출에는 `Mecab` 클래스를 사용하였습니다. 인코딩 등의 문제가 가끔 발생하므로 `Kkma` 클래스에 예외 처리 기능을 추가하여 `myKkma` 클래스를 정의해주었습니다.

```python
class myKkma(Kkma):
     def sentences(self, phrase):
        """
        Sentence detection: 예외처리 추가
        """
        try:
            sentences = self.jki.morphAnalyzer(phrase)
            if not sentences: return []
            return [sentences.get(i).getSentence() for i in range(sentences.size())]
        except:
            return []

kkma, mecab = myKkma(), Mecab()
```
카테고리별로 정의와 관련을 맺는 단어들에 차이가 있을 수 있다고 판단하여, 각 카테고리별로 다른 모델을 훈련하고 그 결과를 저장하는 `modelTrain` 함수를 정의하였습니다. 카테고리마다 문장 단위로 전처리한 데이터들을 데이터 디렉토리에 저장해줍니다. 다음으로 모델을 불러와 정의와 가장 유사한 50개 단어를 반환하는 `loadAndVal` 함수를 정의합니다.

```python
def modelTrain(cat):
    df = data[data.Category == cat] # 카테고리 필터
    filename = cat.replace("/","")
    result = df.Content.apply(lambda x: "|".join(kkma.sentences(x))) # 문장 단위로 구분
    result.reset_index().drop("index", axis=1).to_csv(f"data/{filename}.csv", index=False) # 문장 파싱 결과 저장
    tmp = pd.read_csv(f"data/{filename}.csv").dropna()
    nouns = [mecab.nouns(sentence) for sentence in tmp.Content.apply(lambda x: x.split("|")).sum()] # 명사 추출
    model = Word2Vec(nouns, window=3, min_count=3, size=100, sg=1) # 모델 훈련
    model.save(f"models/skipgram/{filename}.model") # 모델 저장

def loadAndVal(path, topn=50):
    mostSimilar = Word2Vec.load(path).wv.most_similar("정의", topn=topn)
    
    result = pd.DataFrame({
        "category":path.split("/")[-1].split(".")[0],
        "rank":np.arange(1,51),
        "keyword":[word for word, distance in mostSimilar],
        "distance":[distance for word, distance in mostSimilar],
    })
    
    return result
```

이제 실제로 모델을 훈련하고, 결과를 피봇 테이블로 변환하여 확인해줍니다. 

```python
sgModels = [f"./models/skipgram/{filename}" for filename in os.listdir("./models/skipgram/")]
sgResult = pd.concat(list(pd.Series(sgModels).apply(loadAndVal)))
sgResult.pivot(index = "rank", columns = "category", values = "keyword")
```
