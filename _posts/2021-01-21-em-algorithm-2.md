---
layout: post
title: "[ML/Stat] EM Algorithm for latent variable models (2)"
categories: doc
tags: [ml, stat]
comments: true
use_math: true
---

Ïù¥Î≤àÏóêÎäî EM ÏïåÍ≥†Î¶¨Ï¶òÏùÑ ÌÜµÌï¥ÏÑú Ïû†Ïû¨Î≥ÄÏàò Î™®Îç∏ÏùÑ Ïã§Ï†úÎ°ú Ï∂îÏ†ïÌïòÍ≥†, ÌÅ¥Îü¨Ïä§ÌÑ∞ÎßÅÏùÑ ÏãúÌñâÎ¥ÖÎãàÎã§ üòÄ K-means ÌÅ¥Îü¨Ïä§ÌÑ∞ÎßÅÏùò soft assignment Î≤ÑÏ†ÑÏù¥ÎùºÍ≥† ÏÉùÍ∞ÅÌïòÏãúÎ©¥ Ìé∏Ìï† Í≤É Í∞ôÏäµÎãàÎã§! ÏïÑÎûò ÏûêÎ£åÎì§ÏùÑ Ï∞∏Í≥†ÌïòÏó¨ ÏûëÏÑ±Ìïú ÏΩîÎìúÏûÖÎãàÎã§. ÏïÑÏù¥Î¶¨Ïä§ Îç∞Ïù¥ÌÑ∞ÏÖãÏóêÏÑú ÏÇ¨Ïö©Ìïú `plus_plus` Ìï®ÏàòÎäî ÏßÅÏ†ë ÏûëÏÑ±Ìïú Í≤ÉÏù¥ ÏïÑÎãàÎ©∞, ÏïÑÎûò [`centroid_initialization.py`](https://gist.github.com/mmmayo13/3d5c2b12218dfd79acc27c64b3b7dd86#file-centroid_initialization-py) Ïùò ÏΩîÎìúÎ•º ÏÇ¨Ïö©ÌïòÏòÄÏäµÏùÑ ÎØ∏Î¶¨ Î∞ùÌûôÎãàÎã§!

- [Lecture 14 - Expectation-Maximization Algorithms \| Stanford CS229: Machine Learning (Autumn 2018)](https://www.youtube.com/watch?v=rVfZHWTwXSA&t=2192s)
- [27. EM Algorithm for Latent Variable Models](https://www.youtube.com/watch?v=lMShR1vjbUo)
    - [Lecture Slides](https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/13c.EM-algorithm.pdf)
- [CSC 411: Lecture 13: Mixtures of Gaussians and EM](http://nlp.chonbuk.ac.kr/BML/slides_uoft/13_mog.pdf)
- [centroid_initialization.py](https://gist.github.com/mmmayo13/3d5c2b12218dfd79acc27c64b3b7dd86#file-centroid_initialization-py)

## 1. Mixture of Gaussians

ÏßÄÎÇúÎ≤à Ìè¨Ïä§ÌåÖÏóêÏÑú ÏòàÎ•º Îì§ÏóàÎçò Í∞ÄÏö∞ÏãúÏïà ÌòºÌï© Î™®ÌòïÏùò Î™®ÏàòÎ•º Ïã§Ï†úÎ°ú Ï∂îÎ°†Ìï¥Î≥¥Ïûê. [Ïï§ÎìúÎ•ò Ïùë ÍµêÏàòÏùò Í∞ïÏùò](https://www.youtube.com/watch?v=rVfZHWTwXSA&t=2192s) 31Î∂Ñ ÏØ§Î∂ÄÌÑ∞Î•º Ï£ºÎ°ú Ï∞∏Í≥†Ìï¥ÏÑú ÎßåÎì§ÏóàÎã§. `pdf` ÏôÄ `log_likelihood` Îäî Í∞ÅÍ∞Å ÏùºÎ≥ÄÎüâ Ï†ïÍ∑úÎ∂ÑÌè¨Ïùò Î∞ÄÎèÑÏôÄ Î°úÍ∑∏Ïö∞ÎèÑ $\log p(\mathbf{x})$Î•º Í≥ÑÏÇ∞ÌïúÎã§.


```python
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
plt.style.use("ggplot")

# Ï†ïÍ∑úÎ∂ÑÌè¨Ïùò pdf
def pdf(x, mean, std):
    return np.exp(-((x-mean)**2)/(2 * (std**2))) / (std * (2 * np.pi)**0.5)

# Evidence Ïùò Î°úÍ∑∏Ïö∞ÎèÑ
def log_likelihood(x, mu, sigma, p):
    return np.log((pdf(x, mu, sigma) * p).sum(axis=1)).sum().round(4)

# Í¥ÄÏ∏°Í¥éÍ≥º Ïû†Ïû¨Î≥ÄÏàòÏùò Í≤∞Ìï©ÌôïÎ•†Î∞ÄÎèÑ
def p_xz(x, pis, means, sigmas):
    density = pdf(x, means, sigmas)
    return density * pis
#     return np.array([pdf(x, mean, cov) * pi for pi, mean, cov in zip(pis, means, covs)])

np.random.seed(2021)

x1 = np.random.normal(1, 1, 100)  # N(1, 1)
x2 = np.random.normal(10, 3, 50)  # N(10, 3^2)
x = np.append(x1, x2).reshape((-1, 1))
z = np.hstack([np.zeros(shape=(100,), dtype=np.int8), np.ones(shape=(50,), dtype=np.int8)])
df = pd.DataFrame(dict(x=x.flatten(), z=z))

plt.figure(figsize=(16, 4))
plt.subplot(1, 2, 1)
sns.kdeplot(
   data = df, x="x",
   fill=True, common_norm=False, palette="crest",
   alpha=.5, linewidth=0,
)

plt.subplot(1, 2, 2)
sns.kdeplot(
   data = df, x="x", hue="z",
   fill=True, common_norm=False, palette="crest",
   alpha=.5, linewidth=0,
)

plt.suptitle("Univariate Gaussian Mixture", fontsize=15)
plt.show()
```


    
![png](/assets/img/docs/output_4_0.png)
    


### 1.1. Choose initial $\theta^{old}$

EM ÏïåÍ≥†Î¶¨Ï¶òÏùÄ ÏàòÎ†¥Ïù¥ Î≥¥Ïû•ÎêòÏßÄÎßå Í∑∏Í≤ÉÏù¥ Ï†ÑÏó≠ ÏµúÎåÄÎùºÎäî Î≥¥Ïû•Ïù¥ ÏóÜÎã§. Ï¶â Ï¥àÍ∏∞Í∞íÏùÑ Ïñ¥ÎñªÍ≤å ÏÑ§Ï†ïÌïòÎäîÏßÄÏóê Îî∞Îùº ÏïåÍ≥†Î¶¨Ï¶òÏùò ÏàòÎ†¥ Í≤∞Í≥ºÍ∞Ä Îã¨ÎùºÏßà Ïàò ÏûàÎã§. ELBOÏùò Ï†ÑÏó≠ ÏµúÎåÄÍ∞Ä Í≤∞Íµ≠ÏùÄ Î°úÍ∑∏Ïö∞ÎèÑÏùò Ï†ÑÏó≠ ÏµúÎåÄÏôÄ Í∞ôÍ∏∞Îäî ÌïòÏßÄÎßå, ÏïåÍ≥†Î¶¨Ï¶òÏù¥ Ìï≠ÏÉÅ Ï†ÑÏó≠ ÏµúÎåÄÎ•º Ï∞æÏïÑÍ∞à Ïàò ÏûàÎäîÍ∞ÄÎäî Îã§Î•∏ Î¨∏Ï†úÏù¥Îã§. Í∑∏ÎûòÏÑú K-means++ ÏôÄ Í∞ôÏù¥ ÏÑúÎ°ú Ï∂©Î∂ÑÌûà Î©ÄÎ¶¨ Îñ®Ïñ¥ÏßÑ Ï†êÎì§ÏùÑ ÏÑ†ÌÉùÌïòÍ∏∞ ÏúÑÌïú Í∏∞Î≤ïÎì§Ïù¥ Ïó∞Íµ¨ÎêòÏñ¥ÏôîÎã§. Ïù¥Î≤à ÏòàÏ†úÎäî Í∞ÑÎã®Ìïú Ïù∏Ï°∞ Îç∞Ïù¥ÌÑ∞Ïù¥ÎØÄÎ°ú ÎåÄÏ∂© Ï¥àÍ∏∞ÌôîÌï¥Î≥¥Ïûê. Ï∞æÏïÑÏïº Ìï† ÌååÎùºÎØ∏ÌÑ∞Îäî Îëê Ï†ïÍ∑úÎ∂ÑÌè¨Ïùò ÌèâÍ∑†Í≥º Î∂ÑÏÇ∞, Í∑∏Î¶¨Í≥† ÌÅ¥Îü¨Ïä§ÌÑ∞ Ìï†ÎãπÏùò ÌòºÌï©Í≥ÑÏàò(mixing coefficient) Ïù¥Îã§. ÌèâÍ∑†ÏùÄ Í∞ÅÍ∞Å ÏµúÎåÄÍ∞íÍ≥º ÏµúÏÜåÍ∞í, Î∂ÑÏÇ∞ÏùÄ Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤¥Î•º ÌÜµÌï¥ Íµ¨Ìïú Î∂ÑÏÇ∞ÏúºÎ°ú ÎåÄÏ∂© Ï¥àÍ∏∞ÌôîÌñàÎã§. ÌòºÌï©Í≥ÑÏàòÎäî 0.5Ïî©ÏúºÎ°ú ÎèôÏùºÌïòÍ≤å Ï¥àÍ∏∞ÌôîÌñàÎã§.


```python
pis = np.array([[0.5, 0.5]], dtype=np.float64)  # ÌòºÌï©Í≥ÑÏàò pi
means = np.array([x.min(), x.max()])  # Í∞Å Í∞ÄÏö∞ÏãúÏïàÏùò ÌèâÍ∑† mu
stds = np.array([[np.std(x), np.std(x)]], dtype=np.float64)  # Í∞Å Í∞ÄÏö∞ÏãúÏïàÏùò ÌëúÏ§ÄÌé∏Ï∞® sigma
```


```python
logL = log_likelihood(x, means, stds, pis).round(4)  # Ï¥àÍ∏∞ÌôîÎêú ÌååÎùºÎØ∏ÌÑ∞Ïùò Î°úÍ∑∏Ïö∞ÎèÑ
eps = 1e-9  # Î°úÍ∑∏Ïö∞ÎèÑÏùò Í∞úÏÑ†Ïù¥ ÏûÖÏã§Î°†Î≥¥Îã§ ÏûëÏúºÎ©¥ ÏàòÎ†¥ÏúºÎ°ú Í∞ÑÏ£ºÌïòÍ≥† ÏµúÏ†ÅÌôîÎ•º Î©àÏ∂òÎã§
```

### 1.2. Expectation - Maximization

$$
q^{\ast}(z) = p(Z=z \vert x, \theta) \\
\begin{align}
\gamma^{(c)} &:=
\frac {p(Z=c \vert \theta ^{old}) p(x \vert Z=c, \theta ^{old})} {\sum_{j=1}^{k} p(Z=j \vert \theta ^{old}) p(x \vert Z=j, \theta ^{old})}
\end{align}
$$

Î≤†Ïù¥Ï¶à Î£∞ÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ $q(z)$ Î•º ÏúÑÏùò $\gamma^{(c)}$ Ï≤òÎüº ÎÇòÌÉÄÎÇº Ïàò ÏûàÎã§. $\gamma^{(z)}$ Îäî ÌïòÎÇòÏùò Îç∞Ïù¥ÌÑ∞Í∞Ä ÌÅ¥Îü¨Ïä§ÌÑ∞ $z$ Ïóê ÏÜçÌï† Ï°∞Í±¥Î∂ÄÌôïÎ•†ÏùÑ ÎÇòÌÉÄÎÇ∏Îã§. Ïù¥Î•º Ïù¥Ïö©Ìï¥ÏÑú $\log p(\mathbf{x})$ Î•º ÏµúÎåÄÌôîÌïòÎäî ÌååÎùºÎØ∏ÌÑ∞Î•º Íµ¨ÌïòÎ©¥ Îã§ÏùåÍ≥º Í∞ôÎã§. ÏïÑÎûòÏ≤®ÏûêÍ∞Ä Îç∞Ïù¥ÌÑ∞, ÏúÑÏ≤®ÏûêÍ∞Ä ÌÅ¥Îü¨Ïä§ÌÑ∞Î•º ÎÇòÌÉÄÎÇ∏Îã§. ÎØ∏Î∂ÑÏúºÎ°ú MLEÎ•º Íµ¨ÌïòÎäî ÏûêÏÑ∏Ìïú Í≥ºÏ†ïÏùÄ [Ïó¨Í∏∞](http://nlp.chonbuk.ac.kr/BML/slides_uoft/13_mog.pdf)Î•º Ï∞∏Í≥†ÌïòÎ©¥ ÎêúÎã§. 

$$
\begin{align}
& \mu_c^{new}    = \frac{1}{n^{(c)}} \sum_{i=1}^n \gamma_i^{(c)} x_i \\
& \sigma_c^{new} = \frac{1}{n^{(c)}} \sum_{i=1}^n \gamma_i^{(c)} (x_i - \mu_{mle})^2 \\ 
& \pi_c^{new}    = \frac{n^{(c)}}{n} \\
& where \space n^{(c)} = \sum_{i=1}^n \gamma_i^{(c)}  \space and \space n = \sum_{c=1}^k n^{(c)} 
\end{align}
$$


```python
fmt = "EPOCH: {:>5} log-likelihood: {:>.5f} gain: {:>.5f}"
for epoch in range(1, 101):
    
    joint = p_xz(x, pis, means, stds)
    gammas = joint / joint.sum(axis=1, keepdims=True)
    n_c = gammas.sum(axis=0, keepdims=True)

    means = (gammas * x).sum(axis=0, keepdims=True) / n_c
    stds = np.sqrt((gammas * (x - means)**2).sum(axis=0, keepdims=True) / n_c)
    pis = n_c / n_c.sum()

    logL_new = log_likelihood(x, means, stds, pis)
    gain = logL_new - logL
    
    assert gain >= 0

    print(fmt.format(epoch, logL_new, gain))

    if gain < eps:
        
        print("=" * 50, end="\n\n")
        print("Algorithm Converged!!")
        print(fmt.format(epoch, logL_new, gain), end="\n\n")
        print("=" * 50)

        break
    
    else:
        
        logL = logL_new
```

    EPOCH:     1 log-likelihood: -390.07090 gain: 137.82580
    EPOCH:     2 log-likelihood: -362.58360 gain: 27.48730
    EPOCH:     3 log-likelihood: -354.97490 gain: 7.60870
    EPOCH:     4 log-likelihood: -354.27950 gain: 0.69540
    EPOCH:     5 log-likelihood: -354.24200 gain: 0.03750
    EPOCH:     6 log-likelihood: -354.23990 gain: 0.00210
    EPOCH:     7 log-likelihood: -354.23980 gain: 0.00010
    EPOCH:     8 log-likelihood: -354.23980 gain: 0.00000
    ==================================================
    
    Algorithm Converged!!
    EPOCH:     8 log-likelihood: -354.23980 gain: 0.00000
    
    ==================================================


    
![png](/assets/img/docs/output_12_1.png)
    


# 2. Iris Dataset 

Ïù¥Î≤àÏóêÎäî ÏïÑÏù¥Î¶¨Ïä§ Îç∞Ïù¥ÌÑ∞Î°ú Ïã§ÌóòÌï¥Î≥¥Ïûê. Îã§Î≥ÄÎüâ Îç∞Ïù¥ÌÑ∞Ïù¥Í∏∞ Îã§Î≥ÄÎüâ Ï†ïÍ∑úÎ∂ÑÌè¨Ïóê ÎåÄÌïú Ïù¥Ìï¥Í∞Ä Ï°∞Í∏à ÌïÑÏöîÌïòÎã§. Î¨ºÎ°† ÏàòÏãùÎßå ÏùΩÏùÑ Ïàò ÏûàÏúºÎ©¥ Ïôú ÎêòÎäîÏßÄÎäî Î™∞ÎùºÎèÑ Íµ¨ÌòÑÌï† ÏàòÎäî ÏûàÎã§. Ïù¥Î≤àÏóêÎäî Ï¥àÍ∏∞ÌôîÎ•º ÎåÄÏ∂© ÌïòÏßÄ ÏïäÍ≥† Kmeans++ Ïùò Î∞©Î≤ïÏùÑ ÏÇ¨Ïö©ÌñàÎã§. Ï¥àÍ∏∞Ìôî Ìï®ÏàòÏù∏ `plus_plus` Îäî By Matthew MayoÏùò [GIST](https://gist.github.com/mmmayo13/3d5c2b12218dfd79acc27c64b3b7dd86#file-centroid_initialization-py) ÏóêÏÑú Í∞ÄÏ†∏ÏôîÎã§. Í∑∏ Ïô∏ÏóêÎäî Ï†ÑÎ∂Ä ÏßÅÏ†ë ÏûëÏÑ±ÌñàÎã§.


```python
import pandas as pd
from sklearn.datasets import load_iris
```


```python
def pdf(x, mu, sigma):
    
    """pdf of the multivariate gaussian distribution
    
    param x: np.ndarray (n, d)
    param mu: np.ndarray (1, d)
    param sigma: np.ndarray (d, d)
    
    returns: probability densities of x. np.ndarray (n, d)
    """
    
    d = sigma.shape[-1]
    x_m = x - mu
    denominator = np.sqrt((2 * np.pi) ** d * np.linalg.det(sigma))
    numerator = np.exp(
        -np.matmul(
            np.matmul(x_m[:, np.newaxis, :], np.linalg.inv(sigma)),
            x_m[..., np.newaxis]
        ).squeeze() * 0.5
    )
    
    return numerator / denominator
```


```python
def p_xz(x, pis, means, covs):
    
    """compute joint densities  p(x,z). k is the number of clusters.
    
    param x: np.ndarray (n, d)
    param pis: np.ndarray (k,)
    param means: np.ndarray (k, 1, d)
    param covs: (k, d, d)
    
    returns: joint densities of x and z. np.ndarray (k, n)
    """
    
    return np.array([pdf(x, mean, cov) * pi for pi, mean, cov in zip(pis, means, covs)])
```


```python
def log_likelihood(x, pis, means, covs):
    
    """compute log likelihood  log[p(x)].
    
    param x: np.ndarray (n, d)
    param pis: np.ndarray (k,)
    param means: np.ndarray (k, 1, d)
    param covs: (k, d, d)
    
    returns: log likelihood. np.float64
    """
    
    return np.log(p_xz(x, pis, means, covs).sum(axis=0)).sum()
```


```python
def plus_plus(ds, k, random_state=2021):
    
    
    """
    Create cluster centroids using the k-means++ algorithm.
    Parameters
    ----------
    ds : numpy array
        The dataset to be used for centroid initialization.
    k : int
        The desired number of clusters for which centroids are required.
    Returns
    -------
    centroids : numpy array
        Collection of k centroids as a numpy array.
    Inspiration from here: https://stackoverflow.com/questions/5466323/how-could-one-implement-the-k-means-algorithm
    """

    np.random.seed(random_state)
    centroids = [ds[0]]

    for _ in range(1, k):
        dist_sq = np.array([min([np.inner(c-x,c-x) for c in centroids]) for x in ds])
        probs = dist_sq/dist_sq.sum()
        cumulative_probs = probs.cumsum()
        r = np.random.rand()
        
        for j, p in enumerate(cumulative_probs):
            if r < p:
                i = j
                break
        
        centroids.append(ds[i])

    return np.array(centroids)
```


```python
iris = load_iris()
x = iris.get("data")  # 4 X 150 ÌñâÎ†¨
xcols = iris.get("feature_names")  # 4 X 150 ÌñâÎ†¨
y = iris.get("target")  # 4 X 150 ÌñâÎ†¨
```

### 1.1. Choose initial $\theta^{old}$

Ïó≠Ïãú ÌòºÌï©Í≥ÑÏàò, ÌÅ¥Îü¨Ïä§ÌÑ∞Î≥Ñ ÌèâÍ∑†, Í≥µÎ∂ÑÏÇ∞ÌñâÎ†¨ÏùÑ Ï¥àÍ∏∞ÌôîÌï¥Ïïº ÌïúÎã§. ÌòºÌï©Í≥ÑÏàòÏôÄ Í≥µÎ∂ÑÏÇ∞ÌñâÎ†¨ÏùÄ Ïù¥Ï†Ñ ÏòàÏ†úÏóêÏÑúÏôÄ Í∞ôÏùÄ Î∞©ÏãùÏúºÎ°ú Ï¥àÍ∏∞ÌôîÌïòÏòÄÎã§. ÌèâÍ∑†ÏùÄ ÏúÑÏóêÏÑú Ïñ∏Í∏âÌïúÎåÄÎ°ú Kmeans++ Ïùò Î∞©ÏãùÏúºÎ°ú Ï¥àÍ∏∞ÌôîÌïòÏòÄÎã§.


```python
pis = np.array([1/3, 1/3, 1/3])
means = plus_plus(x, 3)
covs = np.repeat(np.cov(x.T)[np.newaxis, ...], 3, axis=0)
```


```python
logL = log_likelihood(x, pis, means, covs)
eps = 1e-19
```

### 1.2. Expectation - Maximization

$$
\begin{align}
\gamma^{(c)} &:=
\frac {p(Z=c \vert \theta ^{old}) p(\mathbf{x} \vert Z=c, \theta ^{old})} {\sum_{j=1}^{k} p(Z=j \vert \theta ^{old}) p(\mathbf{x} \vert Z=j, \theta ^{old})}
\end{align}
$$

$$
\begin{align}
& \mu_c^{new}    = \frac{1}{n^{(c)}} \sum_{i=1}^n \gamma_i^{(c)} \mathbf{x}_i \\
& \Sigma_c^{new} = \frac{1}{n^{(c)}} \sum_{i=1}^n \gamma_i^{(c)} (\mathbf{x}_i - \mu_{mle}) (\mathbf{x}_i - \mu_{mle})^T \\ 
& \pi_c^{new}    = \frac{n^{(c)}}{n} \\
& where \space n^{(c)} = \sum_{i=1}^n \gamma_i^{(c)}  \space and \space n = \sum_{c=1}^k n^{(c)} 
\end{align}
$$

ÏàòÏãùÏÉÅÏúºÎ°úÎäî ÌÅ¨Í≤å Îã¨ÎùºÏßÑ Í≤ÉÏù¥ ÏóÜÎã§. ÌñâÎ†¨/Î≤°ÌÑ∞ Ïó∞ÏÇ∞ÏùÑ ÏßÅÏ†ë Íµ¨ÌòÑÌï¥Î≥∏ Í≤ΩÌóòÏù¥ ÎßéÎã§Î©¥ ÏâΩÍ≤å Ìï† Ïàò ÏûàÏùÑ Í≤É Í∞ôÎã§. Í∞úÏù∏Ï†ÅÏúºÎ°úÎäî ÏßÅÏ†ë ÎÑòÌååÏù¥Î°ú Íµ¨ÌòÑÌï¥Î≥∏ Í≤ΩÌóòÏùÄ ÎßéÏù¥ ÏóÜÏñ¥ÏÑú Í≥†ÏÉùÏùÑ Ï¢Ä ÌïòÎã§Í∞Ä Í≤∞Íµ≠ Î∞òÎ≥µÎ¨∏ÏúºÎ°ú ÌÉÄÌòëÌñàÎã§ üòÖ 


```python
fmt = "EPOCH: {:>5} log-likelihood: {:>.5f} gain: {:>.5f}"
for epoch in range(1, 1001):
    
    joint = p_xz(x, pis, means, covs)[..., np.newaxis]
    gammas = joint / joint.sum(axis=0, keepdims=True)
    N_c = gammas.sum(axis=1)

    means = []
    covs = []
    pis = []
    
    # Í∞Å ÌÅ¥Îü¨Ïä§ÌÑ∞Î≥ÑÎ°ú ÌååÎùºÎØ∏ÌÑ∞Î•º ÏóÖÎç∞Ïù¥Ìä∏ÌïúÎã§
    for gamma, N in zip(gammas, N_c):

        mean = (gamma * x).sum(0, keepdims=True) / N

        x_m = (x - mean)[:, :, np.newaxis]
        cov = np.sum(np.matmul(x_m,  x_m.transpose((0, 2, 1))) * gamma[:, np.newaxis], axis=0) / N

        means.append(mean)
        covs.append(cov)

    pis = N_c / N_c.sum()
    means = np.array(means)
    covs = np.array(covs)

    logL_new = log_likelihood(x, pis, means, covs)
    
    gain = logL_new - logL
    assert gain >= 0

    if epoch % 50 == 0:

        print(fmt.format(epoch, logL_new, gain))

    if logL_new - logL < eps:

        print("=" * 50, end="\n\n")
        print("Algorithm Converged!!")
        print(fmt.format(epoch, logL_new, gain), end="\n\n")
        print("=" * 50)
        break

    else:
        
        logL = logL_new
```

    EPOCH:    50 log-likelihood: -189.42864 gain: 0.00215
    EPOCH:   100 log-likelihood: -189.35420 gain: 0.00039
    EPOCH:   150 log-likelihood: -189.34158 gain: 0.00032
    EPOCH:   200 log-likelihood: -186.80848 gain: 0.18393
    ==================================================
    
    Algorithm Converged!!
    EPOCH:   249 log-likelihood: -186.56946 gain: 0.00000
    
    ==================================================


ÌÅ¥Îü¨Ïä§ÌÑ∞ÎßÅ Í≤∞Í≥ºÍ∞Ä Ïã§Ï†ú ÌíàÏ¢ÖÍ≥º ÏñºÎßàÎÇò ÏùºÏπòÌïòÎäîÏßÄ Ï≤¥ÌÅ¨Ìï¥Î≥¥Ïûê. ÎåÄÏ∂© 17Í∞ú Ï†ïÎèÑÏùò Îç∞Ïù¥ÌÑ∞Í∞Ä Ïã§Ï†ú ÌíàÏ¢ÖÍ≥º ÏùºÏπòÌïòÏßÄ ÏïäÎäî ÌÅ¥Îü¨Ïä§ÌÑ∞Î°ú Î∂ÑÎ•òÎêòÏóàÎã§. ÍΩÉÏûé Í¥ÄÎ†® ÌîºÏ≥êÎ•º ÌÜµÌï¥ÏÑú ÏãúÍ∞ÅÌôîÌï¥Î≥¥Î©¥ 2Î≤à ÌíàÏ¢ÖÏùò ÎåÄÍ∞ÅÏÑ† ÏïÑÎûò Îç∞Ïù¥ÌÑ∞Îì§Ïù¥ 1Î≤à ÌÅ¥Îü¨Ïä§ÌÑ∞Î°ú Î∂ÑÎ•òÎêòÏóàÏùåÏùÑ Ïïå Ïàò ÏûàÎã§. KDE ÌîåÎ°ØÏùÑ ÏÇ¥Ìé¥Î≥¥Î©¥ Ïõê Îç∞Ïù¥ÌÑ∞ÏóêÎèÑ ÎåÄÍ∞ÅÏÑ† ÏúÑÏ™ΩÏóê Î™∞Î†§ÏûàÎäî Í≤ΩÌñ•Ïù¥ Ï°¥Ïû¨ÌïòÎäîÎç∞, Ïù¥Î•º Î∞òÏòÅÌïú Í≤∞Í≥ºÎ°ú Î≥¥Ïù∏Îã§.



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: center;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: center;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
    </tr>
    <!-- <tr>
      <th>species</th>
      <th></th>
      <th></th>
      <th></th>
    </tr> -->
  </thead>
  <tbody style="text-align: center;">
    <tr>
      <th>0</th>
      <td>50</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>49</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>16</td>
      <td>34</td>
    </tr>
  </tbody>
</table>
</div>

    
![png](/assets/img/docs/output_29_1.png)
    

## Ï∞∏Í≥†ÏûêÎ£å


- [Lecture 14 - Expectation-Maximization Algorithms \| Stanford CS229: Machine Learning (Autumn 2018)](https://www.youtube.com/watch?v=rVfZHWTwXSA&t=2192s)
- [27. EM Algorithm for Latent Variable Models](https://www.youtube.com/watch?v=lMShR1vjbUo)
    - [Lecture Slides](https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/13c.EM-algorithm.pdf)
- [CSC 411: Lecture 13: Mixtures of Gaussians and EM](http://nlp.chonbuk.ac.kr/BML/slides_uoft/13_mog.pdf)
- [centroid_initialization.py](https://gist.github.com/mmmayo13/3d5c2b12218dfd79acc27c64b3b7dd86#file-centroid_initialization-py)