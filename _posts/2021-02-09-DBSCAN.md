---
layout: post
title: "[ML/Stat]A Density-Based Algorithm for Discovering Clusters"
categories: docs
tags: ['ml', 'stat']
comments: true
use_math: true
---

*A Density-Based Algorithm for Discovering Clusters(1996)* 을 읽고 공부한 내용입니다 😊


# 1. Intro

상대적으로 큰 공간 데이터에 적합한 클러스터링 알고리즘 **DBSCAN** 을 제안한다. DBSCAN 의 장점은 다음과 같다. 첫째, DBSCAN 은 단 하나의 하이퍼파라미터만을 가지며, 사용자가 적절한 값을 결정할 수 있도록 돕는다. 사실 DBSCAN 에는 \\( Eps, MinPts\\) 라는 두 개의 파라미터가 존재하지만,  \\( MinPts \\) 를 고정하고 \\( Eps \\) 값을 결정하는 방법이 논문 후반부에 제시된다. 둘째, DBSCAN 은 어떤 모양의 클러스터에도 대응할 수 있다. 공간 데이터의 특성상 클러스터는 다양한 형태를 가질 수 있다. 직선 모양으로 기다란 클러스터가 생성될 수도 있고, 한 점을 중심으로 둥글게 모여있을 수도 있다. 직사각형이나 곡선 모양을 갖는 것도 가능하다. DBSCAN 은 밀도에 기반하기 때문에 다양한 클러스터 모양에 대응할 수 있다. 마지막으로, 이전까지 제시된 알고리즘들에 비해서 큰 데이터로의 확장성이 좋다.

## 2. A Density Based Notion of Clusters

이 절에서는 클러스터, 노이즈 등 밀도기반 클러스터링의 기본 개념들을 정의하고, 이에 기반하여 알고리즘을 정당화하는 보조정리들을 소개한다.

> **Definition 1. Eps-neighborhood of a point** 

> The \\(Eps\text{-}neighborhood\\) of a point \\(p\\), denoted by \\( N_{Eps}(p) \\), is defined by   \\( N_{Eps}(p) = \{ q \in D \vert dist(p, q) \leq Eps \} \\) 


    
![png](/assets/img/docs/2021-02-09-DBSCAN_files/2021-02-09-DBSCAN_8_0.png)
    


점 \\( p \\) 로부터 \\( Eps \\) 거리 이내에 있는 점들의 집합을 \\( p \\) 의 입실론 이웃이라고 한다.

> **Definition 2. directly density-reachable** 

> A point \\( p \\) is directly density-reachable from a point a point \\( q \\) wrt \\( Eps, MinPts \\) if 
\\( 1) \space p \in N_{Eps}(q) \\) and  
\\( 2) \space  \vert N_{Eps}(p)\vert \geq MinPts \\) (core point condition)


    
![png](/assets/img/docs/2021-02-09-DBSCAN_files/2021-02-09-DBSCAN_11_0.png)
    


\\( p \\) 가 \\( q \\) 의 입실론 이웃이고 \\( q \\) 의 입실론 이웃인 점이 \\( MinPts \\) 개보다 많으면 \\( p \\) 는 \\( q \\) 로부터 directly density-reachable 하다. 논문에서는 입실론 이웃인 점의 갯수가 \\( MinPts \\) 보다 많은 점들을 코어 포인트라고 말한다.

> **Definition 3. density-reachable** 

> If \\( \exists \\) a chain of points \\( p_1, \cdots, p_n \\) such that \\( p_{i+1} \\) is directly density-reachable from \\( p_{i} \\), then \\( p_{n} \\) is ***density reachable*** from \\( p_{1} \\) 

    
![png](/assets/img/docs/2021-02-09-DBSCAN_files/2021-02-09-DBSCAN_14_0.png)
    


\\( p_{1} \\) 에서 시작해 directly density-reachable 한 포인트들을 연속적으로 타고 가서 \\( p_{n} \\) 에 도착할 수 있다면 \\( p_{n} \\) 은  \\( p_{1} \\) 으로부터 density-reachable 하다. 위 그림에서는 \\( p \\) 에서 출발해 하나의 점을 타고 \\( q \\) 에 도착할 수 있으므로 \\(q \\) 는 \\( p \\) 로부터 density-reachable 하지만, 그 반대는 성립하지 않는다. 

코어 포인트들 간에 density-reachable 관계는 대칭적이다. 코어 포인트 \\( p_{n} \\) 이  \\( p_{1} \\) 으로부터 density-reachable 하다고 가정해보자. 즉 \\( p_{1}, \cdots, p_n  \\) 은 모두 코어 포인트이다. 그렇다면 \\( p_{n-1} \\) 으로부터 \\( p_{n} \\) 에 directly density-reachable 하므로 이 둘은 서로의 입실론 이웃이다. \\( p_n \\) 이 코어 포인트라고 가정하였으므로 \\( p_{n} \\) 으로부터 \\( p_{n-1} \\) 에 directly density-reachable 하다. 이 과정을 반복하면 \\( p_1 \\) 에 도달할 수 있다.

> **Definition 4. density-connected** 

> If \\( \exists \\) a point \\( o \\) such that both \\( p \\) and \\( q \\) are density-reachable from \\( o \\), then \\( p \\) is ***dentisy-connected*** to \\( q \\)


    
![png](/assets/img/docs/2021-02-09-DBSCAN_files/2021-02-09-DBSCAN_17_0.png)
    


\\( p \\) 와 \\( q \\) 가 동일한 점 \\( o \\) 로부터 density-reachable 하다면 \\( p \\) 와 \\( q \\) 는 density-connected 하다. 이 관계는 위에 제시된 정의들과 달리 경계 포인트들 간에도 대칭적으로 성립하는 관계이다. 그림에 주어진 \\( p \\) 와 \\( q \\) 는 모두 경계 포인트들이지만 모두 \\( o \\) 로부터 density-reachable 하다.

> **Definition 5. cluster** 

> Let \\( D \\) be a database of points. A ***cluster*** \\( C \\) is a non-empty subset of \\( D \\) satisfying the following conditions 
\\( 1) \space \forall p,q : \\) if \\( p \in C \\) and \\( q \\) is density-reachable from \\( p \\), then \\( q \in C \\) (Maximality)
\\( 1) \space \forall p,q \in C: \\) \\( p \\) is density-connected to \\( q \\) (Connectivity)


> **Definition 6. noise** 

> Let \\( C_1, \cdots C_k \\) be the clusters of the database \\( D \\). Then we define the ***noise*** as the set of points in the database \\( D \\) not belog to ant cluster \\( C_i \\), i.e, \\( noise = \{ p \in D \vert \forall i: p \notin C_i \} \\)


    
![png](/assets/img/docs/2021-02-09-DBSCAN_files/2021-02-09-DBSCAN_21_0.png)
    


1) ***Maximality***: \\( p \\)가 클러스터 \\( C \\) 에 속한다면, \\( p \\) 로부터 density-reachable 한 모든 점들이 \\( C \\)에 속한다.

2) ***Connectivity***: 하나의 클러스터에 속하는 모든 점들은 서로 density-connected 하다.

위 두 가지 조건을 만족하는 점들의 집합을 **클러스터**라고 정의하고, 어떤 클러스터에도 속하지 않는 점들을 **노이즈**라고 정의한다. 클러스터는 공집합이 아니므로 최소한 하나의 점 \\( o \\) 를 포함한다. 둘째 조건에 의해 \\( o \\) 는 클러스터 내의 점들과 density-connected 하다. Density-connected 조건을 만족시키기 위해서 클러스터는 최소한 하나의 코어 포인트를 포함해야 \\( p \\) 를 포함해야 한다. 코어 포인트의 입실론 이웃은 최소한 \\( MinPts \\) 개의 원소로 이루어지므로, 클러스터 역시 최소한 \\( MinPts \\) 개의 원소를 갖는다. 

클러스터가 코어 포인트 \\( p \\) 를 포함한다면, 첫째 조건에 따라 \\( p \\) 로부터 density-reachable 한 점들 역시 모두 같은 클러스터에 속한다. 사실 첫째 조건이 만족된다면 둘째 조건은 자동으로 만족된다. \\( p \\) 로부터 density-reachable 한 점들을 모두 모아 놓는다면, 각각의 점들은 \\( p \\) 를 매개로 density-connected 하기 때문이다. 이에 따라 첫번째 보조정리가 도출된다. 하나의 코어 포인트 \\( p \\) 로부터 density-reachable 한 점들을 모두 모아놓으면 그것이 클러스터의 정의를 만족한다는 내용이다. 

> **Lemma 1.** 

> Let \\( p \\) be a point in \\( D \\) and \\(\vert  N_{eps}(p) \vert \geq MinPts \\). Then the  set \\( O = \{ o \vert o \in D, \space o \text{ is density-reachable from from } p \} \\) is a cluster.

클러스터에서 임의의 점을 선택해 density-reachable 한 점들을 모두 모아 놓으면 동일한 클러스터가 된다. 클러스터 \\( C \\) 가 서로 다른 코어 포인트 \\( p \\) 와 \\( q \\) 를 포함한다고 가정해보자. 첫째 조건에 의해 \\( p \\) 와 \\( q \\) 는 서로 density-reachable 하다. 그렇다면 \\( p \\) 로부터 density-reachable 한 점 \\( o \\) 는 \\( q \\) 로부터도 density-rechable 하다. \\( q, \cdots p, \cdots,  o \\) 와 같이 directly density-reachable 한 점들의 체인이 반드시 존재하기 때문이다. 반대의 경우에도 마찬가지이다. 즉 임의의 점 \\( o \in C \\) 는 임의의 코어 포인트 \\( p \in C \\) 로부터 density-reachable 하다.

> **Lemma 2.** 

> Let \\( C \\) be a cluster and let \\( p \\) be any point in \\( C \\) with \\( \vert  N_{eps}(p) \vert \geq MinPts  \\). Then \\( C \\) equals to the set \\( O = \{ o \vert o \in D, \space o \text{ is density-reachable from from } p \} \\).

## 4. DBSCAN

### 4.1. The Algorithm

위에 제시된 보조정리들에 의하면, 데이터베이스에서 순차적으로 하나의 점을 골라 그로부터 클러스터를 확장하는 방식으로 모든 클러스터를 찾아낼 수 있다.  만약 선택된 점이 코어 포인트라면 density-reachable 한 점들을 모두 찾아 클러스터를 생성한다. 보조정리에 따라 하나의 클러스터 내에서는 어떤 점을 선택해 클러스터를 확장하건 동일한 결과를 얻을 수 있으므로 점들의 순서는 무관하다. 만약 선택된 점이 코어 포인트가 아니라면 다음 포인트로 넘어간다. 사실 이러한 진행에서 주변 포인트들은 두 개 이상의 클러스터에 동시에 속할 수 있다. 하지만 이는 매우 드문 경우이므로, 먼저 발견된 클러스터에 할당하는 등 적절하게 처리해주면 되겠다.

이제 알고리즘을 파이썬 코드로 옮겨보자. 가능한 의사코드를 그대로 표현하려고 했는데, 능력 부족으로 완전히 동일하게 표현하지는 못했다 😅 일단 하나의 점 주변에서 유클리드 거리로 입실론 이내에 존재하는 점들을 검색하는 `regionQuery` 함수를 먼저 정의하자. R-tree 와 같은 자료구조를 사용하면 클러스터링을 \\(O ( n\log n ) \\) 시간에 완료할 수 있다는데, 역시 능력 부족으로 브루트 포스 방식을 택했다.


```python
def regionQuery(setOfPoints, point, eps):
    mask = ((setOfPoints.x - point.x)**2 + (setOfPoints.y - point.y)**2) <= eps**2
    return setOfPoints[mask].index
```

다음은 메인 루프를 포함하는 `dbscan` 함수이다. 데이터 집합과 하이퍼파라미터를 받아서 클러스터링 결과를 반환한다. 데이터를 순회하면서 입실론 이웃을 검색해 코어 포인트인지를 판단하고, 코어 포인트가 아닌 경우에는 우선 노이즈로 둔다. 만약 다른 코어 포인트로부터 density-reachable 하다는 사실이 밝혀진다면 그 때 값을 변경해주도록 한다. 코어 포인트인 경우에는 `expandCluster` 함수를 호출하여 해당 점에서 클러스터를 확장한다. 논문의 의사코드에서는 포인트의 입실론 이웃을 구하는 기능부터 `expandCluster` 함수가 담당한다. 여기에서는 시드를 구하는 과정을 함수 밖으로 빼내고, `expandCluster` 는 클러스터를 확장하는 기능만 담당하도록 했다.


```python
def dbscan(setOfPoints, eps, minPts):
    
    clusterId = 0  
    setOfPoints = setOfPoints.assign(clId = clusterId)  # UNCLASSIFIED: 0
    
    for i in tqdm(range(setOfPoints.shape[0])):  
        
        point = setOfPoints.loc[i]  # 점들을 돌면서
        if point.clId == 0:  # 아직 클러스터 할당이 없는 점이면
            
            seeds = regionQuery(setOfPoints, point, eps)  # 입실론 이웃을 검색한다
            
            if seeds.size < minPts:  # 코어 포인트가 아니라면
                setOfPoints.loc[seeds, "clId"] = -1  # 일단 노이즈로 두고 넘어간다
            
            else:  # 코어 포인트라면
                clusterId += 1
                setOfPoints = expandCluster(  # 해당 포인트에서 클러스터를 확장한다
                    setOfPoints = setOfPoints,
                    point = point,
                    seeds = seeds,
                    clId = clusterId,
                    eps = eps,
                    minPts = minPts
                )
    
    return setOfPoints.clId.values
```

`expandCluster` 는 데이터 집합과 하나의 점, 그리고 입실론 이웃을 받아 점을 중심으로 클러스터를 확장한다. 시드들은 모두 주어진 포인트로부터 directly density-reachable 하므로 같은 클러스터에 속한다. 시드들 중에 또다른 코어 포인트가 존재할 수 있으므로, 시드를 순차적으로 돌면서 코어 포인트 여부를 파악한다. 만약 코어 포인트라면 입실론 이웃들에 같은 클러스터를 할당하고, 그중에서 코어 포인트가 될 수 있는 점들을 다시 시드의 마지막에 추가한다. 더이상 확장 가능한 시드가 없어질 때까지 이 과정을 반복한다. 언제나 느끼지만 알고리즘을 말이나 글로 설명한다는게 참 쉽지가 않은 것 같다.


```python
def expandCluster(setOfPoints, point, seeds, clId, eps, minPts):
    
    # point 의 입실론 이웃들은 모두 directly density-reachable 하므로 같은 클러스터로 할당한다
    setOfPoints.loc[seeds, "clId"] = clId  
    
    # 코어 포인트 시드들을 돌면서 클러스터를 확장한다
    seeds = seeds.drop(pd.Index([point.name]))  # point 는 이미 확장했으므로 제외한다
    
    while not seeds.empty:  # 확장할 시드가 남아있다면
        
        currentP = setOfPoints.loc[seeds[0]]  # 시드에서 첫 포인트 선택
        result = regionQuery(setOfPoints, currentP, eps)  # 입실론 이웃 검색
        
        
        if result.size >= minPts:  # 코어 포인트이면 여기에서 다시 확장 가능한 점들을 찾아야 한다.
            
            mask = setOfPoints.loc[result, "clId"] == 0  # 아직 클러스터가 없는 점들은 확장 가능성이 있다
            seeds = seeds.append(result[mask])  # 확장 가능성이 있는 애들은 시드에 추가
            setOfPoints.loc[result, "clId"] = clId  # 클러스터를 할당해준다. 이전에 노이즈로 판단된 점들도 이 과정에서 클러스터를 할당받는다.
        
        seeds = seeds[1:]  # 확장이 완료된 시드는 제외한다
        
    return setOfPoints
```

이제 만들어진 알고리즘을 테스트해보자. [사이킷런 문서](https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html#sphx-glr-auto-examples-cluster-plot-cluster-comparison-py) 와 동일하게 원 모양, 반달 모양으로 분포하는 데이터를 각각 생성하였다. 하이퍼파라미터 역시 문서와 동일하게 부여하고 클러스터를 적절하게 잡아내는지 확인해보았다. 사이킷런 클래스보다는 훨씬 느리지만 아무튼 결과는 잘 잡아내는 것을 볼 수 있다. 


```python
n_samples = 1500
circles = datasets.make_circles(n_samples=n_samples, factor=.5, noise=.05)
moons = datasets.make_moons(n_samples=n_samples, noise=.05)
datas = [circles, moons]
```


```python
plt.figure(figsize=(12,6))
for i in range(2):
    sops = pd.DataFrame(data=datas[i][0], columns=["x", "y"])
    cls_labels = dbscan(sops, eps=0.15, minPts=5)
    plt.subplot(1, 2, i+1)
    sns.scatterplot(data=sops, x="x", y="y", hue=cls_labels.astype(str))
    plt.axis('equal')
else:
    plt.show()
```


    
![png](/assets/img/docs/2021-02-09-DBSCAN_files/2021-02-09-DBSCAN_32_2.png)
    


### 4.2. Determining the parameters Eps and MinPts

위에서 살펴본것처럼 DBSCAN 은 \\( Eps, MinPts \\) 라는 두 개의 하이퍼파라미터를 갖는다. 여기에서는 이 하이퍼파라미터들을 결정하는 방법에 대해 알아본다. 먼저 \\( k\text{-}dist \\) 라는 함수를 정의해보자.  \\( k\text{-}dist \\) 는 한 점 \\( p \\) 를 받아서 \\( p \\) 로부터 \\( k \\) 번쨰로 가까운 점까지의 거리를 반환하는 함수이다. 가령 \\( 4\text{-}dist(p) \\) 는 \\( p \\) 로부터 네번째로 가까운 점까지의 거리를 반환한다. 

이제 인조 데이터를 생성한 후 \\( 4\text{-}dist \\) 를 측정해서 정렬해보자. 이를 그래프로 그리면 아래의 두번째 그림과 같은 모양이 된다. 대략 100번째 데이터 근처에서 엘보가 나타나는 것으로 보인다. 100번째 데이터의 \\( 4\text{-}dist \\) 는 약 `0.0412` 정도가 된다. 만약 \\( Eps=0.0412, \space MinPts=4 \\) 의 하이퍼파라미터로 DBSCAN 을 실행한다면 점선 우측의 데이터들은 모두 코어 포인트가 될 것이다. \\( 4\text{-}dist \\) 함수값이 입실론보다 작다는 것은 이 점들의 입실론 이웃이 최소 4개의 점을 포함함을 의미한다. 반대로 점선 좌측의 데이터들은 경계 포인트 혹은 노이즈가 될 것이다. 

이제 우리는 커브의 엘보 근처에서 입실론을 조절하는 방식으로 적절한 파라미터를 결정할 수 있다. \\( MinPts \\) 는 4로 고정하는데, 실험 결과 \\( k\text{-}dist \\) 의 함수값은 \\( k > 4\\) 인 경우에는 큰 차이가 없기 때문이라고 한다. 즉 \\( MinPts=4 \\) 로 고정한 상태에서 \\( sorted \space k\text{-}dist \space graph \\) 를 그려 적절한 \\( Eps \\) 값을 결정할 수 있다.


```python
def k_dist(setOfPoints, k=4):
    setOfPoints = setOfPoints[['x', 'y']].values
    dist_matrix = np.linalg.norm(setOfPoints[:, None, :] - setOfPoints[None, :, :], axis=-1)
    dist = np.array(list(map(lambda x: x[x.argsort()][k-1], dist_matrix)))
    return np.array(dist)

sops = datasets.make_blobs(n_samples=1500, random_state=8)[0]
sops = pd.DataFrame(sops, columns=["x", "y"])
sops = sops.assign(kdist = k_dist(sops)).sort_values("kdist", ascending=False).reset_index()
```


    
![png](/assets/img/docs/2021-02-09-DBSCAN_files/2021-02-09-DBSCAN_36_1.png)
    


## 5. 서울시 어린이 교통사고 클러스터링

사실 수리사회학이라는 학부 수업 과제에서 DBSCAN 을 활용했던 적이 있다. 그때는 `sklearn` 을 썼는데, 동일한 작업을 직접 구현한 `dbscan` 함수로 시행해보았다. 사용된 데이터는 TAAS GIS 시스템에서 수집한 어린이 교통사고 데이터이다. 지리 데이터 전처리/후처리 관련 내용이 많아 자세한 코드는 생략하였다 😅 


<iframe src="/assets/img/docs/2021-02-09-DBSCAN_files/cluster.html" style="width:100%; height:600px; border:none">

</iframe>